{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#https://www.youtube.com/watch?v=APmF6qE3Vjc (17:50)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from skimage.io import imread, imshow\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import scipy.misc\n",
    "\n",
    "from subprocess import check_output\n",
    "import glob\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Type 1 cervices is 248.\n",
      "The number of Type 2 cervices is 780.\n",
      "The number of Type 3 cervices is 450.\n",
      "The number of Test cervices is 504.\n",
      "The total number of all cervices is 1982.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagepath</th>\n",
       "      <th>filetype</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/keerat/dev/AOSResearch/resources/Train/...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/keerat/dev/AOSResearch/resources/Train/...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/keerat/dev/AOSResearch/resources/Train/...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/keerat/dev/AOSResearch/resources/Train/...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/keerat/dev/AOSResearch/resources/Train/...</td>\n",
       "      <td>jpg</td>\n",
       "      <td>Type_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           imagepath filetype    type\n",
       "0  /Users/keerat/dev/AOSResearch/resources/Train/...      jpg  Type_1\n",
       "1  /Users/keerat/dev/AOSResearch/resources/Train/...      jpg  Type_1\n",
       "2  /Users/keerat/dev/AOSResearch/resources/Train/...      jpg  Type_1\n",
       "3  /Users/keerat/dev/AOSResearch/resources/Train/...      jpg  Type_1\n",
       "4  /Users/keerat/dev/AOSResearch/resources/Train/...      jpg  Type_1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting each path to the folders for each type of cervix\n",
    "basepath = \"/Users/keerat/dev/AOSResearch/resources/Train/\"\n",
    "path_type1 = \"/Users/keerat/dev/AOSResearch/resources/Train/Type_1\"\n",
    "path_type2 = \"/Users/keerat/dev/AOSResearch/resources/Train/Type_2\"\n",
    "path_type3 = \"/Users/keerat/dev/AOSResearch/resources/Train/Type_3\"\n",
    "paths = [path_type1, path_type2, path_type3]\n",
    "\n",
    "#glob will sort through each folder(Type 1, 2, 3) and return the information in it as a list\n",
    "type1_cervix_images = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Train/Type_1/*.jpg\")\n",
    "type2_cervix_images = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Train/Type_2/*.jpg\")\n",
    "type3_cervix_images = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Train/Type_3/*.jpg\")\n",
    "test_cervix_images = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Test/*.jpg\")\n",
    "\n",
    "#all_cervix_images holds all the data\n",
    "all_cervix_images = type1_cervix_images + type2_cervix_images + type3_cervix_images + test_cervix_images\n",
    "\n",
    "#checking the number of each cervix type \n",
    "print(\"The number of Type 1 cervices is {}.\".format((len(type1_cervix_images))))\n",
    "print(\"The number of Type 2 cervices is {}.\".format((len(type2_cervix_images))))\n",
    "print(\"The number of Type 3 cervices is {}.\".format((len(type3_cervix_images))))\n",
    "print(\"The number of Test cervices is {}.\".format((len(test_cervix_images))))\n",
    "print(\"The total number of all cervices is {}.\".format((len(all_cervix_images))))\n",
    "\n",
    "#The data has been transported inton a dataframe\n",
    "all_cervix_images = pd.DataFrame({'imagepath': all_cervix_images})\n",
    "all_cervix_images['filetype'] = all_cervix_images.apply(lambda row: row.imagepath.split(\".\")[-1], axis=1)\n",
    "all_cervix_images['type'] = all_cervix_images.apply(lambda row: row.imagepath.split(\"/\")[-2], axis=1)\n",
    "all_cervix_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "(3264, 4160)\n",
      "<class 'str'>\n",
      "(3264, 3917)\n",
      "<class 'str'>\n",
      "(3120, 4160)\n",
      "<class 'str'>\n",
      "(3264, 2448)\n",
      "<class 'str'>\n",
      "(2340, 4160)\n",
      "<class 'str'>\n",
      "(3096, 4128)\n",
      "<class 'str'>\n",
      "(2448, 3264)\n",
      "<class 'str'>\n",
      "(480, 640)\n",
      "<class 'str'>\n",
      "(2322, 4128)\n"
     ]
    }
   ],
   "source": [
    "sizes = []\n",
    "sizes_files = []\n",
    "for filepath in all_cervix_images['imagepath']:\n",
    "    if(Image.open(filepath).size not in sizes):\n",
    "        sizes.append(Image.open(filepath).size)\n",
    "        sizes_files.append(filepath)\n",
    "for a in sizes_files:\n",
    "    print(type(a))\n",
    "    print(Image.open(a).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes (rows, columns)\n",
      "(3264, 4160)\n",
      "(3264, 3917)\n",
      "(3120, 4160)\n",
      "(3264, 2448)\n",
      "(2340, 4160)\n",
      "(3096, 4128)\n",
      "(2448, 3264)\n",
      "(480, 640)\n",
      "(2322, 4128)\n",
      "Zero-padded sizes (columns, rows)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n",
      "(4160, 3264, 3)\n"
     ]
    }
   ],
   "source": [
    "#3264 = max width\n",
    "#4160 = max height\n",
    "changed_size = []\n",
    "print(\"Original Sizes (rows, columns)\")\n",
    "for t in sizes_files:\n",
    "    print(Image.open(t).size)\n",
    "    if (Image.open(t).size[1] % 2 == 1):\n",
    "        top = (int)((4160 - Image.open(t).size[1])/2 + 1)\n",
    "        bottom = (int)((4160 - Image.open(t).size[1])/2)\n",
    "        left = (int)((3264 - Image.open(t).size[0])/2)\n",
    "        right = (int)((3264 - Image.open(t).size[0])/2)\n",
    "    elif (Image.open(t).size[0] % 2 == 1):\n",
    "        top = (int)((4160 - Image.open(t).size[1])/2)\n",
    "        bottom = (int)((4160 - Image.open(t).size[1])/2)\n",
    "        left = (int)((3264 - Image.open(t).size[0])/2 + 1)\n",
    "        right = (int)((3264 - Image.open(t).size[0])/2)\n",
    "    else:\n",
    "        top = (int)((4160 - Image.open(t).size[1])/2)\n",
    "        bottom = (int)((4160 - Image.open(t).size[1])/2)\n",
    "        left = (int)((3264 - Image.open(t).size[0])/2)\n",
    "        right = (int)((3264 - Image.open(t).size[0])/2)\n",
    "    #changed_size.append(cv2.copyMakeBorder( cv2.imread(t), top, bottom, left, right, cv2.BORDER_CONSTANT))\n",
    "    changed_size.append(cv2.copyMakeBorder( cv2.imread(t), top, bottom, left, right, cv2.BORDER_CONSTANT, value = [0,0,0]))\n",
    "    image = Image.fromarray(cv2.copyMakeBorder( cv2.imread(t), top, bottom, left, right, cv2.BORDER_CONSTANT, value = [0,0,0]))\n",
    "    image.save(t)\n",
    "print(\"Zero-padded sizes (columns, rows)\")    \n",
    "for a in changed_size:\n",
    "    print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "#When dealing with high-dimensional inputs such as images, \n",
    "#it is impractical to connect neurons to all neurons in the previous volume. \n",
    "#Instead, we will connect each neuron to only a local region of the input volume. \n",
    "#The spatial extent of this connectivity is a hyperparameter called the receptive field \n",
    "#of the neuron (equivalently this is the filter size). \n",
    "#smaller size than input\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "#more filters, featuer map will b\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size_x = 3264\n",
    "img_size_y = 4160\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size_x * img_size_y\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size_x, img_size_y)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "#https://en.wikipedia.org/wiki/Channel_(digital_image)\n",
    "#channels mean number of primary colors\n",
    "num_channels = 3\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the first images from the test-set.\n",
    "data = []\n",
    "for a in all_cervix_images['imagepath']:\n",
    "    data.append(plt.imread(a))\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = data.a\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    #equivalent to y intercept\n",
    "    #constant value carried over across matrix math\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 2x2 max-pooling.\n",
    "\n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    # This format is determined by the TensorFlow API.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(shape=shape)\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases\n",
    "\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = \\\n",
    "    new_conv_layer(input=x_image,\n",
    "                   num_input_channels=num_channels,\n",
    "                   filter_size=filter_size1,\n",
    "                   num_filters=num_filters1,\n",
    "                   use_pooling=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
