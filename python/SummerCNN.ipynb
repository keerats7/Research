{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter, ImageStat, Image, ImageDraw\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_multi(path):\n",
    "    #Input: file path of image\n",
    "    #Output: [path, {'size': size of image at path}]\n",
    "    try:\n",
    "        im_stats_im_ = Image.open(path)\n",
    "        return [path, {'size': im_stats_im_.size}]\n",
    "    except:\n",
    "        print(path)\n",
    "        return [path, {'size': [0,0]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_stats(im_stats_df): \n",
    "    #Input: dataframe of training images (type, image, path)\n",
    "    #Output: dataframe of training images (type, image, path, size)\n",
    "    im_stats_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(im_multi, im_stats_df['path'])\n",
    "    #p.map(f, [x, y, z]) returns a list [f[x], f[y], f[z]]\n",
    "    #For all paths in the inputted dataframe im_stats_df, they are passed through im_multi\n",
    "    #ret holds [[path, {'size': size of image at path}], ...]\n",
    "    for i in range(len(ret)):\n",
    "        im_stats_d[ret[i][0]] = ret[i][1]\n",
    "    im_stats_df['size'] = im_stats_df['path'].map(lambda x: ' '.join(str(s) for s in \n",
    "                                                                     im_stats_d[x]['size']))\n",
    "    #Adds additional column to original dataframe and formats size as 3264 4160\n",
    "    return im_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    #Input: file path of image\n",
    "    #Output: [original path, resized image]\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (64, 64), cv2.INTER_LINEAR) #INTER_LINEAR is algorithm \n",
    "    #to downsize image\n",
    "    #I could try using INTER_AREA as, according to the URL below, could be better\n",
    "    #http://tanbakuchi.com/posts/comparison-of-openv-interpolation-algorithms/\n",
    "    return [path, resized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image_features(paths):\n",
    "    #Input: list of paths\n",
    "    #Output: list of resized images that have been transposed for Conv2d layer\n",
    "    imf_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(get_im_cv2, paths)\n",
    "    #ret holds a list: [[image path, resized image], ...]\n",
    "    for i in range(len(ret)):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "        #imf_d[image path in ret] = resized image\n",
    "    ret = []\n",
    "    fdata = [imf_d[f] for f in paths]\n",
    "    #fdata holds a list: [resized image, ...]\n",
    "    fdata = np.array(fdata, dtype=np.uint8)\n",
    "    #fdata is now a numbpy array of ints\n",
    "    fdata = fdata.transpose((0, 3, 1, 2))\n",
    "    #Usually its (2, 0, 1) since it changes the image from (0, 1, 2)->(width, height, channel)\n",
    "    #to (2, 0, 1)->(channel, width, height) for the Conv2d layer, \n",
    "    #but since we have 4 dimensions, it gets bumped up one to (3, 1, 2);\n",
    "    #I don't get why it has 4 dimensions tho (https://skymind.ai/wiki/convolutional-network)\n",
    "    fdata = fdata.astype('float32')\n",
    "    fdata = fdata / 255\n",
    "    #fdata now has values between 0 and 1\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in training set: % 1478\n",
      "loading train data\n",
      "train data loaded\n",
      "['Type_1' 'Type_2' 'Type_3']\n",
      "loading test data\n",
      "test data loaded\n"
     ]
    }
   ],
   "source": [
    "train = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Train/**/*.jpg\")\n",
    "#train is an array holding all of the path files in the training set\n",
    "print(\"Number of files in training set: %\", len(train))\n",
    "train = pd.DataFrame([[p.split('/')[7],p.split('/')[8],p] for p in train], columns = \n",
    "                     ['type','image','path'])[::1]\n",
    "#train is a dataframe holding the type (ex. \"Type_1\"), image name (ex. \"0.jpg\"), and file path\n",
    "#(ex. \"/Users/keerat/dev/AOSResearch/resources/Train/Type_1/0.jpg\")\n",
    "train = im_stats(train)\n",
    "#train now has additional column with size (ex. 3264 4160)\n",
    "train = train[train['size'] != '0 0'].reset_index(drop=True) #corrupt images removed\n",
    "#train now has an additional column with the index reset to 0, 1, 2, 3... instead of 0, 5, 10..\n",
    "print(\"loading train data\")\n",
    "train_data = normalize_image_features(train['path'])\n",
    "#train_data holds a usable set of training images for the CNN\n",
    "print(\"train data loaded\")\n",
    "np.save('train.npy', train_data, allow_pickle=True, fix_imports=True)\n",
    "#train.npy is a file that has all of the image arrays in train_data\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_target = le.fit_transform(train['type'].values) \n",
    "#train_target holds type of each image in train\n",
    "#Type_1 = 0, Type_2 = 1, Type_3 = 2\n",
    "#For example, if the 40th image in train is Type_1, then train_target[40] = 0\n",
    "print(le.classes_)  \n",
    "np.save('train_target.npy', train_target, allow_pickle=True, fix_imports=True)\n",
    "#train_target.npy is a file that has all values of train_target\n",
    "\n",
    "test = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/test/*.jpg\")\n",
    "#test is an array holding all of the path files in the test set\n",
    "test = pd.DataFrame([[p.split('/')[7],p] for p in test], columns = ['image','path']) [::1]\n",
    "#test is a dataframe holding the image name (ex. \"0.jpg\"), and file path\n",
    "#(ex. \"/Users/keerat/dev/AOSResearch/resources/test/0.jpg\")\n",
    "print(\"loading test data\")\n",
    "test_data = normalize_image_features(test['path'])\n",
    "#test_data holds a usable set of test images for the CNN\n",
    "np.save('test.npy', test_data, allow_pickle=True, fix_imports=True)\n",
    "#test.npy is a file that has all of the image arrays in test_data\n",
    "print(\"test data loaded\")\n",
    "test_id = test.image.values\n",
    "np.save('test_id.npy', test_id, allow_pickle=True, fix_imports=True)\n",
    "#test_id.npy is a file that has all of the image names (ex. '0.jpg) in test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keerat/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.set_floatx('float32')\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train.npy')\n",
    "train_target = np.load('train_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(opt_='adamax'):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th', \n",
    "                            input_shape=(3, 64, 64))) \n",
    "    #Could try different input shape\n",
    "    \n",
    "    #Activation='relu' to discover nonlinear patterns of data\n",
    "    #dim_ordering = 'th' to match (0, 3, 1, 2) images were transposed to\n",
    "    #Four 3x3 filters\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Sets a fraction of rate of input units to 0 to prevent overfitting\n",
    "    model.add(Flatten())\n",
    "    #Creates 1D feature vector for Dense layers\n",
    "    model.add(Dense(12, activation='tanh')) #Classifies\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation='softmax')) #Classifies\n",
    "\n",
    "    model.compile(optimizer=opt_, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #Compiles all layers of model with optimizer, loss function, and metrics (to evaluate \n",
    "    #performance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanImages():\n",
    "    datagen = ImageDataGenerator(rotation_range=0.3, zoom_range=0.3)\n",
    "    #rotation_range = random rotation of images up to 0.3 degrees\n",
    "    #zoom_range = random zoom of images up to a scale of 0.3\n",
    "    datagen.fit(train_data)\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitAndPredict(): #Runs data through model\n",
    "    print(\"cleaning images\")\n",
    "    datagen=cleanImages() #datagen now points to the parameters in cleanImages()\n",
    "    print(\"images cleaned\")\n",
    "    \n",
    "    model = create_model() #model holds CNN model\n",
    "    x_train,x_val_train,y_train,y_val_train = train_test_split(train_data,train_target,\n",
    "                                                               test_size=0.4, random_state=17)\n",
    "    #x_train = training set of images (60% of original training set)\n",
    "    #x_val_train = validation set of images (40 % of original training set)\n",
    "    #y_train = types for training images\n",
    "    #y_val_train = types for validation images\n",
    "    \n",
    "    #Training set is used to fit the parameters using back prop\n",
    "    #Validation set is used to fine tune parameters to create a final model\n",
    "    #Test set is used to assess model's performance\n",
    "    print(\"fitting data\")\n",
    "    model.fit_generator(datagen.flow(x_train,y_train, batch_size=15, shuffle=True), \n",
    "                        nb_epoch=200, samples_per_epoch=len(x_train), \n",
    "                        verbose=2, validation_data=(x_val_train, y_val_train))\n",
    "    #Training set is augmented real-time with datagen.flow\n",
    "    #CNN processes images not one at a time, but in batches.  With batch_size = 15, one batch \n",
    "    #is 15 x 3 x 64 x 64.  The batch can't be too big, or else the machine can't handle it,\n",
    "    #and it can't be too small or else there will be no variance within the batch.\n",
    "    print(\"data fitted in model\")\n",
    "    loss, accuracy = model.evaluate(train_data, train_target)\n",
    "    print('loss: ', loss, '\\naccuracy: ', accuracy)\n",
    "    test_data = np.load('test.npy')\n",
    "    test_id = np.load('test_id.npy')\n",
    "    print(\"creating predictions\")\n",
    "    predictions = model.predict_proba(test_data)\n",
    "    #Runs test_data through model and returns probablibity of it being each type\n",
    "    print(\"predictions made\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(isTrue): #Runs CNN on test images\n",
    "    pred=fitAndPredict()\n",
    "    print(\"creating test file\")\n",
    "    df = pd.DataFrame(pred, columns=['Type_1','Type_2','Type_3']) #Instantiates dataframe\n",
    "    df['image_name'] = test_id #image_name holds the .jpg file name\n",
    "    if (isTrue): #if(True), it will create a .csv file with the dataframe\n",
    "        df.to_csv('test.csv', index=False)\n",
    "        print(\"Test file created in users/keerat/...\")\n",
    "    else: #if(False), it will just show the dataframe\n",
    "        print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning images\n",
      "images cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", input_shape=(3, 64, 64..., data_format=\"channels_first\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=2, validation_data=(array([[[..., steps_per_epoch=59, epochs=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 10s - loss: 1.0011 - acc: 0.5377 - val_loss: 1.0484 - val_acc: 0.5203\n",
      "Epoch 2/200\n",
      " - 9s - loss: 1.0103 - acc: 0.5199 - val_loss: 1.0376 - val_acc: 0.5203\n",
      "Epoch 3/200\n",
      " - 8s - loss: 0.9756 - acc: 0.5403 - val_loss: 1.0226 - val_acc: 0.5203\n",
      "Epoch 4/200\n",
      " - 8s - loss: 0.9860 - acc: 0.5154 - val_loss: 1.0380 - val_acc: 0.5186\n",
      "Epoch 5/200\n",
      " - 7s - loss: 0.9759 - acc: 0.5513 - val_loss: 1.0192 - val_acc: 0.5186\n",
      "Epoch 6/200\n",
      " - 7s - loss: 0.9773 - acc: 0.5356 - val_loss: 1.0293 - val_acc: 0.5203\n",
      "Epoch 7/200\n",
      " - 7s - loss: 0.9596 - acc: 0.5490 - val_loss: 1.0215 - val_acc: 0.5203\n",
      "Epoch 8/200\n",
      " - 7s - loss: 0.9538 - acc: 0.5346 - val_loss: 1.0102 - val_acc: 0.4966\n",
      "Epoch 9/200\n",
      " - 6s - loss: 0.9534 - acc: 0.5314 - val_loss: 1.0048 - val_acc: 0.5084\n",
      "Epoch 10/200\n",
      " - 7s - loss: 0.9499 - acc: 0.5267 - val_loss: 1.0015 - val_acc: 0.5051\n",
      "Epoch 11/200\n",
      " - 6s - loss: 0.9186 - acc: 0.5603 - val_loss: 0.9754 - val_acc: 0.4983\n",
      "Epoch 12/200\n",
      " - 7s - loss: 0.9086 - acc: 0.5931 - val_loss: 0.9809 - val_acc: 0.5084\n",
      "Epoch 13/200\n",
      " - 7s - loss: 0.9036 - acc: 0.5504 - val_loss: 0.9883 - val_acc: 0.5017\n",
      "Epoch 14/200\n",
      " - 6s - loss: 0.8856 - acc: 0.5931 - val_loss: 0.9654 - val_acc: 0.5169\n",
      "Epoch 15/200\n",
      " - 8s - loss: 0.8736 - acc: 0.6055 - val_loss: 0.9759 - val_acc: 0.5152\n",
      "Epoch 16/200\n",
      " - 8s - loss: 0.8937 - acc: 0.5798 - val_loss: 0.9799 - val_acc: 0.5034\n",
      "Epoch 17/200\n",
      " - 9s - loss: 0.8523 - acc: 0.6010 - val_loss: 0.9689 - val_acc: 0.5068\n",
      "Epoch 18/200\n",
      " - 8s - loss: 0.8837 - acc: 0.5866 - val_loss: 0.9621 - val_acc: 0.5017\n",
      "Epoch 19/200\n",
      " - 7s - loss: 0.8723 - acc: 0.5877 - val_loss: 0.9603 - val_acc: 0.5270\n",
      "Epoch 20/200\n",
      " - 7s - loss: 0.8747 - acc: 0.5753 - val_loss: 0.9512 - val_acc: 0.5236\n",
      "Epoch 21/200\n",
      " - 8s - loss: 0.8415 - acc: 0.6270 - val_loss: 0.9560 - val_acc: 0.5270\n",
      "Epoch 22/200\n",
      " - 7s - loss: 0.8734 - acc: 0.5944 - val_loss: 0.9695 - val_acc: 0.5135\n",
      "Epoch 23/200\n",
      " - 7s - loss: 0.8419 - acc: 0.6020 - val_loss: 0.9545 - val_acc: 0.5422\n",
      "Epoch 24/200\n",
      " - 8s - loss: 0.8364 - acc: 0.6225 - val_loss: 0.9565 - val_acc: 0.5220\n",
      "Epoch 25/200\n",
      " - 9s - loss: 0.8320 - acc: 0.6304 - val_loss: 0.9691 - val_acc: 0.5372\n",
      "Epoch 26/200\n",
      " - 7s - loss: 0.8617 - acc: 0.6034 - val_loss: 0.9594 - val_acc: 0.5338\n",
      "Epoch 27/200\n",
      " - 7s - loss: 0.8693 - acc: 0.5661 - val_loss: 0.9561 - val_acc: 0.5422\n",
      "Epoch 28/200\n",
      " - 7s - loss: 0.8315 - acc: 0.6136 - val_loss: 0.9521 - val_acc: 0.5304\n",
      "Epoch 29/200\n",
      " - 6s - loss: 0.8576 - acc: 0.5755 - val_loss: 0.9624 - val_acc: 0.5422\n",
      "Epoch 30/200\n",
      " - 6s - loss: 0.8520 - acc: 0.6101 - val_loss: 0.9614 - val_acc: 0.5236\n",
      "Epoch 31/200\n",
      " - 7s - loss: 0.8216 - acc: 0.6316 - val_loss: 0.9560 - val_acc: 0.5304\n",
      "Epoch 32/200\n",
      " - 6s - loss: 0.8841 - acc: 0.5798 - val_loss: 0.9421 - val_acc: 0.5355\n",
      "Epoch 33/200\n",
      " - 9s - loss: 0.8324 - acc: 0.6194 - val_loss: 0.9477 - val_acc: 0.5372\n",
      "Epoch 34/200\n",
      " - 9s - loss: 0.8328 - acc: 0.6259 - val_loss: 0.9353 - val_acc: 0.5625\n",
      "Epoch 35/200\n",
      " - 6s - loss: 0.8030 - acc: 0.6227 - val_loss: 0.9563 - val_acc: 0.5338\n",
      "Epoch 36/200\n",
      " - 6s - loss: 0.8141 - acc: 0.6136 - val_loss: 0.9555 - val_acc: 0.5338\n",
      "Epoch 37/200\n",
      " - 7s - loss: 0.8036 - acc: 0.6462 - val_loss: 0.9638 - val_acc: 0.5338\n",
      "Epoch 38/200\n",
      " - 7s - loss: 0.8448 - acc: 0.6044 - val_loss: 0.9462 - val_acc: 0.5405\n",
      "Epoch 39/200\n",
      " - 9s - loss: 0.8011 - acc: 0.6181 - val_loss: 0.9577 - val_acc: 0.5405\n",
      "Epoch 40/200\n",
      " - 8s - loss: 0.8301 - acc: 0.6147 - val_loss: 0.9415 - val_acc: 0.5304\n",
      "Epoch 41/200\n",
      " - 8s - loss: 0.7980 - acc: 0.6338 - val_loss: 0.9470 - val_acc: 0.5389\n",
      "Epoch 42/200\n",
      " - 7s - loss: 0.8382 - acc: 0.6360 - val_loss: 0.9446 - val_acc: 0.5321\n",
      "Epoch 43/200\n",
      " - 6s - loss: 0.7767 - acc: 0.6519 - val_loss: 0.9813 - val_acc: 0.5355\n",
      "Epoch 44/200\n",
      " - 6s - loss: 0.8368 - acc: 0.6158 - val_loss: 0.9629 - val_acc: 0.5253\n",
      "Epoch 45/200\n",
      " - 7s - loss: 0.8271 - acc: 0.6239 - val_loss: 0.9439 - val_acc: 0.5439\n",
      "Epoch 46/200\n",
      " - 7s - loss: 0.8124 - acc: 0.6205 - val_loss: 0.9525 - val_acc: 0.5422\n",
      "Epoch 47/200\n",
      " - 7s - loss: 0.8053 - acc: 0.6541 - val_loss: 0.9488 - val_acc: 0.5389\n",
      "Epoch 48/200\n",
      " - 7s - loss: 0.7919 - acc: 0.6406 - val_loss: 0.9535 - val_acc: 0.5405\n",
      "Epoch 49/200\n",
      " - 6s - loss: 0.8107 - acc: 0.6036 - val_loss: 0.9615 - val_acc: 0.5405\n",
      "Epoch 50/200\n",
      " - 6s - loss: 0.7982 - acc: 0.6228 - val_loss: 0.9601 - val_acc: 0.5389\n",
      "Epoch 51/200\n",
      " - 7s - loss: 0.8251 - acc: 0.6420 - val_loss: 0.9505 - val_acc: 0.5236\n",
      "Epoch 52/200\n",
      " - 6s - loss: 0.7947 - acc: 0.6262 - val_loss: 0.9503 - val_acc: 0.5287\n",
      "Epoch 53/200\n",
      " - 7s - loss: 0.7833 - acc: 0.6598 - val_loss: 0.9571 - val_acc: 0.5253\n",
      "Epoch 54/200\n",
      " - 7s - loss: 0.7773 - acc: 0.6496 - val_loss: 0.9554 - val_acc: 0.5524\n",
      "Epoch 55/200\n",
      " - 7s - loss: 0.7844 - acc: 0.6440 - val_loss: 0.9966 - val_acc: 0.5236\n",
      "Epoch 56/200\n",
      " - 7s - loss: 0.8066 - acc: 0.6283 - val_loss: 0.9678 - val_acc: 0.5389\n",
      "Epoch 57/200\n",
      " - 7s - loss: 0.7834 - acc: 0.6429 - val_loss: 0.9636 - val_acc: 0.5405\n",
      "Epoch 58/200\n",
      " - 8s - loss: 0.8027 - acc: 0.6281 - val_loss: 0.9626 - val_acc: 0.5068\n",
      "Epoch 59/200\n",
      " - 8s - loss: 0.7736 - acc: 0.6801 - val_loss: 0.9535 - val_acc: 0.5372\n",
      "Epoch 60/200\n",
      " - 8s - loss: 0.7899 - acc: 0.6214 - val_loss: 0.9641 - val_acc: 0.5372\n",
      "Epoch 61/200\n",
      " - 7s - loss: 0.7960 - acc: 0.6327 - val_loss: 0.9668 - val_acc: 0.5287\n",
      "Epoch 62/200\n",
      " - 7s - loss: 0.7881 - acc: 0.6485 - val_loss: 0.9578 - val_acc: 0.5287\n",
      "Epoch 63/200\n",
      " - 7s - loss: 0.7746 - acc: 0.6508 - val_loss: 0.9591 - val_acc: 0.5473\n",
      "Epoch 64/200\n",
      " - 7s - loss: 0.7951 - acc: 0.6295 - val_loss: 0.9744 - val_acc: 0.5338\n",
      "Epoch 65/200\n",
      " - 7s - loss: 0.7551 - acc: 0.6508 - val_loss: 0.9702 - val_acc: 0.5287\n",
      "Epoch 66/200\n",
      " - 7s - loss: 0.7546 - acc: 0.6621 - val_loss: 0.9786 - val_acc: 0.5118\n",
      "Epoch 67/200\n",
      " - 8s - loss: 0.7884 - acc: 0.6462 - val_loss: 0.9762 - val_acc: 0.5355\n",
      "Epoch 68/200\n",
      " - 9s - loss: 0.8144 - acc: 0.6567 - val_loss: 0.9808 - val_acc: 0.5203\n",
      "Epoch 69/200\n",
      " - 8s - loss: 0.7718 - acc: 0.6520 - val_loss: 0.9830 - val_acc: 0.5355\n",
      "Epoch 70/200\n",
      " - 6s - loss: 0.8079 - acc: 0.6331 - val_loss: 0.9868 - val_acc: 0.5169\n",
      "Epoch 71/200\n",
      " - 8s - loss: 0.7938 - acc: 0.6408 - val_loss: 0.9861 - val_acc: 0.5203\n",
      "Epoch 72/200\n",
      " - 8s - loss: 0.7360 - acc: 0.6677 - val_loss: 0.9664 - val_acc: 0.5389\n",
      "Epoch 73/200\n",
      " - 8s - loss: 0.7911 - acc: 0.6341 - val_loss: 0.9773 - val_acc: 0.5355\n",
      "Epoch 74/200\n",
      " - 7s - loss: 0.7633 - acc: 0.6420 - val_loss: 0.9692 - val_acc: 0.5203\n",
      "Epoch 75/200\n",
      " - 6s - loss: 0.7885 - acc: 0.6564 - val_loss: 0.9738 - val_acc: 0.5236\n",
      "Epoch 76/200\n",
      " - 8s - loss: 0.7684 - acc: 0.6589 - val_loss: 0.9874 - val_acc: 0.5034\n",
      "Epoch 77/200\n",
      " - 7s - loss: 0.7897 - acc: 0.6496 - val_loss: 0.9749 - val_acc: 0.5338\n",
      "Epoch 78/200\n",
      " - 8s - loss: 0.7639 - acc: 0.6567 - val_loss: 0.9733 - val_acc: 0.5220\n",
      "Epoch 79/200\n",
      " - 9s - loss: 0.7572 - acc: 0.6734 - val_loss: 0.9696 - val_acc: 0.5389\n",
      "Epoch 80/200\n",
      " - 8s - loss: 0.7513 - acc: 0.6362 - val_loss: 0.9767 - val_acc: 0.5270\n",
      "Epoch 81/200\n",
      " - 8s - loss: 0.7615 - acc: 0.6578 - val_loss: 0.9880 - val_acc: 0.5338\n",
      "Epoch 82/200\n",
      " - 8s - loss: 0.7743 - acc: 0.6555 - val_loss: 0.9747 - val_acc: 0.5338\n",
      "Epoch 83/200\n",
      " - 8s - loss: 0.7301 - acc: 0.6904 - val_loss: 0.9866 - val_acc: 0.5236\n",
      "Epoch 84/200\n",
      " - 7s - loss: 0.7572 - acc: 0.6578 - val_loss: 0.9823 - val_acc: 0.5186\n",
      "Epoch 85/200\n",
      " - 7s - loss: 0.7279 - acc: 0.6914 - val_loss: 0.9753 - val_acc: 0.5203\n",
      "Epoch 86/200\n",
      " - 9s - loss: 0.7658 - acc: 0.6598 - val_loss: 0.9791 - val_acc: 0.5372\n",
      "Epoch 87/200\n",
      " - 7s - loss: 0.7188 - acc: 0.6915 - val_loss: 0.9915 - val_acc: 0.5321\n",
      "Epoch 88/200\n",
      " - 7s - loss: 0.7441 - acc: 0.6755 - val_loss: 0.9897 - val_acc: 0.5203\n",
      "Epoch 89/200\n",
      " - 8s - loss: 0.7642 - acc: 0.6610 - val_loss: 0.9918 - val_acc: 0.5135\n",
      "Epoch 90/200\n",
      " - 10s - loss: 0.7484 - acc: 0.6734 - val_loss: 0.9880 - val_acc: 0.5338\n",
      "Epoch 91/200\n",
      " - 9s - loss: 0.7457 - acc: 0.6454 - val_loss: 1.0014 - val_acc: 0.5372\n",
      "Epoch 92/200\n",
      " - 8s - loss: 0.7218 - acc: 0.7005 - val_loss: 1.0014 - val_acc: 0.5321\n",
      "Epoch 93/200\n",
      " - 8s - loss: 0.7213 - acc: 0.6971 - val_loss: 0.9949 - val_acc: 0.5169\n",
      "Epoch 94/200\n",
      " - 9s - loss: 0.7321 - acc: 0.6846 - val_loss: 0.9961 - val_acc: 0.5118\n",
      "Epoch 95/200\n",
      " - 9s - loss: 0.7703 - acc: 0.6723 - val_loss: 0.9869 - val_acc: 0.5304\n",
      "Epoch 96/200\n",
      " - 9s - loss: 0.7390 - acc: 0.6880 - val_loss: 1.0208 - val_acc: 0.5270\n",
      "Epoch 97/200\n",
      " - 8s - loss: 0.7269 - acc: 0.6892 - val_loss: 1.0007 - val_acc: 0.5203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      " - 7s - loss: 0.7459 - acc: 0.6700 - val_loss: 1.0043 - val_acc: 0.5338\n",
      "Epoch 99/200\n",
      " - 7s - loss: 0.7319 - acc: 0.6835 - val_loss: 1.0038 - val_acc: 0.5236\n",
      "Epoch 100/200\n",
      " - 8s - loss: 0.7348 - acc: 0.6893 - val_loss: 1.0072 - val_acc: 0.5084\n",
      "Epoch 101/200\n",
      " - 8s - loss: 0.7269 - acc: 0.6735 - val_loss: 1.0010 - val_acc: 0.5118\n",
      "Epoch 102/200\n",
      " - 7s - loss: 0.7297 - acc: 0.6825 - val_loss: 1.0259 - val_acc: 0.5236\n",
      "Epoch 103/200\n",
      " - 7s - loss: 0.7806 - acc: 0.6612 - val_loss: 1.0041 - val_acc: 0.5152\n",
      "Epoch 104/200\n",
      " - 7s - loss: 0.7298 - acc: 0.6677 - val_loss: 1.0051 - val_acc: 0.5321\n",
      "Epoch 105/200\n",
      " - 8s - loss: 0.6897 - acc: 0.7174 - val_loss: 1.0195 - val_acc: 0.5355\n",
      "Epoch 106/200\n",
      " - 9s - loss: 0.7365 - acc: 0.6655 - val_loss: 1.0160 - val_acc: 0.5270\n",
      "Epoch 107/200\n",
      " - 11s - loss: 0.7107 - acc: 0.6948 - val_loss: 1.0334 - val_acc: 0.5321\n",
      "Epoch 108/200\n",
      " - 7s - loss: 0.7214 - acc: 0.6903 - val_loss: 1.0062 - val_acc: 0.5372\n",
      "Epoch 109/200\n",
      " - 8s - loss: 0.7457 - acc: 0.6835 - val_loss: 1.0068 - val_acc: 0.5321\n",
      "Epoch 110/200\n",
      " - 8s - loss: 0.7274 - acc: 0.6903 - val_loss: 1.0206 - val_acc: 0.5220\n",
      "Epoch 111/200\n",
      " - 7s - loss: 0.7329 - acc: 0.6691 - val_loss: 1.0147 - val_acc: 0.5405\n",
      "Epoch 112/200\n",
      " - 7s - loss: 0.7454 - acc: 0.6533 - val_loss: 1.0154 - val_acc: 0.5203\n",
      "Epoch 113/200\n",
      " - 7s - loss: 0.7276 - acc: 0.6872 - val_loss: 1.0251 - val_acc: 0.5304\n",
      "Epoch 114/200\n",
      " - 7s - loss: 0.7401 - acc: 0.6714 - val_loss: 1.0312 - val_acc: 0.5068\n",
      "Epoch 115/200\n",
      " - 7s - loss: 0.6949 - acc: 0.6948 - val_loss: 1.0232 - val_acc: 0.5456\n",
      "Epoch 116/200\n",
      " - 7s - loss: 0.6938 - acc: 0.6869 - val_loss: 1.0299 - val_acc: 0.5169\n",
      "Epoch 117/200\n",
      " - 7s - loss: 0.6886 - acc: 0.6994 - val_loss: 1.0348 - val_acc: 0.5321\n",
      "Epoch 118/200\n",
      " - 7s - loss: 0.7221 - acc: 0.6824 - val_loss: 1.0091 - val_acc: 0.5253\n",
      "Epoch 119/200\n",
      " - 7s - loss: 0.6885 - acc: 0.7039 - val_loss: 1.0210 - val_acc: 0.5253\n",
      "Epoch 120/200\n",
      " - 7s - loss: 0.7546 - acc: 0.6714 - val_loss: 1.0224 - val_acc: 0.5321\n",
      "Epoch 121/200\n",
      " - 7s - loss: 0.7090 - acc: 0.6906 - val_loss: 1.0341 - val_acc: 0.5135\n",
      "Epoch 122/200\n",
      " - 7s - loss: 0.7090 - acc: 0.6881 - val_loss: 1.0495 - val_acc: 0.5220\n",
      "Epoch 123/200\n",
      " - 7s - loss: 0.7399 - acc: 0.6813 - val_loss: 1.0397 - val_acc: 0.5253\n",
      "Epoch 124/200\n",
      " - 7s - loss: 0.7509 - acc: 0.6623 - val_loss: 1.0315 - val_acc: 0.5236\n",
      "Epoch 125/200\n",
      " - 7s - loss: 0.6855 - acc: 0.6994 - val_loss: 1.0515 - val_acc: 0.5101\n",
      "Epoch 126/200\n",
      " - 7s - loss: 0.7055 - acc: 0.6914 - val_loss: 1.0114 - val_acc: 0.5220\n",
      "Epoch 127/200\n",
      " - 7s - loss: 0.7034 - acc: 0.6971 - val_loss: 1.0471 - val_acc: 0.5270\n",
      "Epoch 128/200\n",
      " - 7s - loss: 0.6827 - acc: 0.6926 - val_loss: 1.0248 - val_acc: 0.5135\n",
      "Epoch 129/200\n",
      " - 7s - loss: 0.7334 - acc: 0.6859 - val_loss: 1.0332 - val_acc: 0.5389\n",
      "Epoch 130/200\n",
      " - 6s - loss: 0.6611 - acc: 0.7185 - val_loss: 1.0278 - val_acc: 0.5236\n",
      "Epoch 131/200\n",
      " - 7s - loss: 0.6975 - acc: 0.6793 - val_loss: 1.0334 - val_acc: 0.5338\n",
      "Epoch 132/200\n",
      " - 7s - loss: 0.7093 - acc: 0.7096 - val_loss: 1.0326 - val_acc: 0.5405\n",
      "Epoch 133/200\n",
      " - 7s - loss: 0.6842 - acc: 0.7086 - val_loss: 1.0254 - val_acc: 0.5355\n",
      "Epoch 134/200\n",
      " - 7s - loss: 0.7160 - acc: 0.6883 - val_loss: 1.0233 - val_acc: 0.5304\n",
      "Epoch 135/200\n",
      " - 7s - loss: 0.6690 - acc: 0.7231 - val_loss: 1.0518 - val_acc: 0.5304\n",
      "Epoch 136/200\n",
      " - 7s - loss: 0.7287 - acc: 0.6756 - val_loss: 1.0293 - val_acc: 0.5084\n",
      "Epoch 137/200\n",
      " - 7s - loss: 0.7116 - acc: 0.6793 - val_loss: 1.0236 - val_acc: 0.5220\n",
      "Epoch 138/200\n",
      " - 7s - loss: 0.6912 - acc: 0.7028 - val_loss: 1.0636 - val_acc: 0.5118\n",
      "Epoch 139/200\n",
      " - 11s - loss: 0.7353 - acc: 0.6871 - val_loss: 1.0565 - val_acc: 0.5321\n",
      "Epoch 140/200\n",
      " - 11s - loss: 0.6855 - acc: 0.7027 - val_loss: 1.0453 - val_acc: 0.5253\n",
      "Epoch 141/200\n",
      " - 6s - loss: 0.7186 - acc: 0.6960 - val_loss: 1.0385 - val_acc: 0.5169\n",
      "Epoch 142/200\n",
      " - 7s - loss: 0.6658 - acc: 0.7220 - val_loss: 1.0428 - val_acc: 0.5169\n",
      "Epoch 143/200\n",
      " - 8s - loss: 0.6856 - acc: 0.7085 - val_loss: 1.0358 - val_acc: 0.5253\n",
      "Epoch 144/200\n",
      " - 8s - loss: 0.7047 - acc: 0.6914 - val_loss: 1.0613 - val_acc: 0.5118\n",
      "Epoch 145/200\n",
      " - 9s - loss: 0.7019 - acc: 0.6815 - val_loss: 1.0433 - val_acc: 0.5203\n",
      "Epoch 146/200\n",
      " - 8s - loss: 0.7063 - acc: 0.6704 - val_loss: 1.0731 - val_acc: 0.5287\n",
      "Epoch 147/200\n",
      " - 9s - loss: 0.6717 - acc: 0.7130 - val_loss: 1.0585 - val_acc: 0.5338\n",
      "Epoch 148/200\n",
      " - 8s - loss: 0.6760 - acc: 0.7016 - val_loss: 1.0527 - val_acc: 0.5135\n",
      "Epoch 149/200\n",
      " - 9s - loss: 0.6943 - acc: 0.6881 - val_loss: 1.0740 - val_acc: 0.5203\n",
      "Epoch 150/200\n",
      " - 9s - loss: 0.6874 - acc: 0.6947 - val_loss: 1.0551 - val_acc: 0.5389\n",
      "Epoch 151/200\n",
      " - 9s - loss: 0.6818 - acc: 0.7152 - val_loss: 1.0751 - val_acc: 0.5270\n",
      "Epoch 152/200\n",
      " - 8s - loss: 0.7568 - acc: 0.6872 - val_loss: 1.0497 - val_acc: 0.5405\n",
      "Epoch 153/200\n",
      " - 8s - loss: 0.6608 - acc: 0.7061 - val_loss: 1.0951 - val_acc: 0.5304\n",
      "Epoch 154/200\n",
      " - 9s - loss: 0.6863 - acc: 0.6938 - val_loss: 1.0549 - val_acc: 0.5270\n",
      "Epoch 155/200\n",
      " - 8s - loss: 0.7032 - acc: 0.6914 - val_loss: 1.0481 - val_acc: 0.5270\n",
      "Epoch 156/200\n",
      " - 8s - loss: 0.6862 - acc: 0.7049 - val_loss: 1.0736 - val_acc: 0.5389\n",
      "Epoch 157/200\n",
      " - 8s - loss: 0.6944 - acc: 0.7085 - val_loss: 1.0649 - val_acc: 0.5405\n",
      "Epoch 158/200\n",
      " - 9s - loss: 0.6687 - acc: 0.7210 - val_loss: 1.0633 - val_acc: 0.5068\n",
      "Epoch 159/200\n",
      " - 8s - loss: 0.7005 - acc: 0.6972 - val_loss: 1.0767 - val_acc: 0.5101\n",
      "Epoch 160/200\n",
      " - 8s - loss: 0.6827 - acc: 0.6982 - val_loss: 1.0608 - val_acc: 0.5253\n",
      "Epoch 161/200\n",
      " - 8s - loss: 0.7039 - acc: 0.6759 - val_loss: 1.0455 - val_acc: 0.5270\n",
      "Epoch 162/200\n",
      " - 8s - loss: 0.6605 - acc: 0.6995 - val_loss: 1.0789 - val_acc: 0.5118\n",
      "Epoch 163/200\n",
      " - 8s - loss: 0.6658 - acc: 0.7220 - val_loss: 1.0594 - val_acc: 0.5372\n",
      "Epoch 164/200\n",
      " - 8s - loss: 0.7036 - acc: 0.6748 - val_loss: 1.0640 - val_acc: 0.5338\n",
      "Epoch 165/200\n",
      " - 8s - loss: 0.6321 - acc: 0.7490 - val_loss: 1.1094 - val_acc: 0.5355\n",
      "Epoch 166/200\n",
      " - 9s - loss: 0.6923 - acc: 0.6904 - val_loss: 1.0742 - val_acc: 0.5372\n",
      "Epoch 167/200\n",
      " - 8s - loss: 0.6626 - acc: 0.7241 - val_loss: 1.0679 - val_acc: 0.5253\n",
      "Epoch 168/200\n",
      " - 9s - loss: 0.6418 - acc: 0.7367 - val_loss: 1.0811 - val_acc: 0.5287\n",
      "Epoch 169/200\n",
      " - 9s - loss: 0.6808 - acc: 0.7006 - val_loss: 1.0614 - val_acc: 0.5203\n",
      "Epoch 170/200\n",
      " - 8s - loss: 0.6795 - acc: 0.7073 - val_loss: 1.0628 - val_acc: 0.5372\n",
      "Epoch 171/200\n",
      " - 8s - loss: 0.6567 - acc: 0.7174 - val_loss: 1.0767 - val_acc: 0.5220\n",
      "Epoch 172/200\n",
      " - 9s - loss: 0.6400 - acc: 0.7265 - val_loss: 1.0862 - val_acc: 0.5169\n",
      "Epoch 173/200\n",
      " - 9s - loss: 0.6763 - acc: 0.6982 - val_loss: 1.0752 - val_acc: 0.5220\n",
      "Epoch 174/200\n",
      " - 9s - loss: 0.6051 - acc: 0.7513 - val_loss: 1.0893 - val_acc: 0.5422\n",
      "Epoch 175/200\n",
      " - 9s - loss: 0.6336 - acc: 0.7311 - val_loss: 1.1009 - val_acc: 0.5169\n",
      "Epoch 176/200\n",
      " - 11s - loss: 0.6580 - acc: 0.7242 - val_loss: 1.0917 - val_acc: 0.5270\n",
      "Epoch 177/200\n",
      " - 11s - loss: 0.6439 - acc: 0.7287 - val_loss: 1.0918 - val_acc: 0.5084\n",
      "Epoch 178/200\n",
      " - 9s - loss: 0.6636 - acc: 0.7163 - val_loss: 1.0818 - val_acc: 0.5270\n",
      "Epoch 179/200\n",
      " - 15s - loss: 0.6841 - acc: 0.6872 - val_loss: 1.0935 - val_acc: 0.5287\n",
      "Epoch 180/200\n",
      " - 13s - loss: 0.6547 - acc: 0.7208 - val_loss: 1.0976 - val_acc: 0.5186\n",
      "Epoch 181/200\n",
      " - 10s - loss: 0.6724 - acc: 0.7177 - val_loss: 1.0798 - val_acc: 0.5236\n",
      "Epoch 182/200\n",
      " - 10s - loss: 0.6309 - acc: 0.7234 - val_loss: 1.1036 - val_acc: 0.5355\n",
      "Epoch 183/200\n",
      " - 12s - loss: 0.6622 - acc: 0.7096 - val_loss: 1.0885 - val_acc: 0.5287\n",
      "Epoch 184/200\n",
      " - 12s - loss: 0.6649 - acc: 0.6987 - val_loss: 1.0870 - val_acc: 0.5203\n",
      "Epoch 185/200\n",
      " - 10s - loss: 0.6291 - acc: 0.7389 - val_loss: 1.0803 - val_acc: 0.5355\n",
      "Epoch 186/200\n",
      " - 10s - loss: 0.6537 - acc: 0.7243 - val_loss: 1.0821 - val_acc: 0.5270\n",
      "Epoch 187/200\n",
      " - 9s - loss: 0.6634 - acc: 0.7224 - val_loss: 1.1054 - val_acc: 0.5304\n",
      "Epoch 188/200\n",
      " - 9s - loss: 0.6261 - acc: 0.7378 - val_loss: 1.0954 - val_acc: 0.5372\n",
      "Epoch 189/200\n",
      " - 9s - loss: 0.6708 - acc: 0.6985 - val_loss: 1.0821 - val_acc: 0.5270\n",
      "Epoch 190/200\n",
      " - 9s - loss: 0.6395 - acc: 0.7109 - val_loss: 1.1050 - val_acc: 0.5304\n",
      "Epoch 191/200\n",
      " - 9s - loss: 0.6323 - acc: 0.7412 - val_loss: 1.0743 - val_acc: 0.5220\n",
      "Epoch 192/200\n",
      " - 8s - loss: 0.6267 - acc: 0.7547 - val_loss: 1.0952 - val_acc: 0.5253\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.6063 - acc: 0.7423 - val_loss: 1.0980 - val_acc: 0.5169\n",
      "Epoch 194/200\n",
      " - 10s - loss: 0.6378 - acc: 0.7030 - val_loss: 1.1254 - val_acc: 0.5355\n",
      "Epoch 195/200\n",
      " - 11s - loss: 0.6439 - acc: 0.7321 - val_loss: 1.1095 - val_acc: 0.5287\n",
      "Epoch 196/200\n",
      " - 9s - loss: 0.6115 - acc: 0.7367 - val_loss: 1.0966 - val_acc: 0.5321\n",
      "Epoch 197/200\n",
      " - 8s - loss: 0.6581 - acc: 0.6953 - val_loss: 1.1022 - val_acc: 0.5304\n",
      "Epoch 198/200\n",
      " - 8s - loss: 0.6492 - acc: 0.7220 - val_loss: 1.1107 - val_acc: 0.5169\n",
      "Epoch 199/200\n",
      " - 8s - loss: 0.6614 - acc: 0.7030 - val_loss: 1.0952 - val_acc: 0.5304\n",
      "Epoch 200/200\n",
      " - 8s - loss: 0.6082 - acc: 0.7414 - val_loss: 1.0923 - val_acc: 0.5186\n",
      "data fitted in model\n",
      "1478/1478 [==============================] - 7s 5ms/step\n",
      "loss:  0.7218181119074841 \n",
      "accuracy:  0.7083897157515499\n",
      "creating predictions\n",
      "predictions made\n",
      "creating test file\n",
      "       Type_1    Type_2    Type_3 image_name\n",
      "0    0.070165  0.431213  0.498623      0.jpg\n",
      "1    0.013187  0.028032  0.958781      1.jpg\n",
      "2    0.042994  0.250152  0.706854     10.jpg\n",
      "3    0.021184  0.738681  0.240135    100.jpg\n",
      "4    0.453360  0.104348  0.442292    101.jpg\n",
      "5    0.132350  0.368227  0.499423    102.jpg\n",
      "6    0.640849  0.244890  0.114261    103.jpg\n",
      "7    0.013550  0.851791  0.134659    104.jpg\n",
      "8    0.034843  0.139683  0.825474    105.jpg\n",
      "9    0.556793  0.143805  0.299402    106.jpg\n",
      "10   0.073965  0.829744  0.096291    107.jpg\n",
      "11   0.017628  0.237673  0.744699    108.jpg\n",
      "12   0.045081  0.413682  0.541238    109.jpg\n",
      "13   0.077956  0.501761  0.420283     11.jpg\n",
      "14   0.038781  0.832903  0.128316    110.jpg\n",
      "15   0.034107  0.767190  0.198702    111.jpg\n",
      "16   0.024777  0.176418  0.798805    112.jpg\n",
      "17   0.057401  0.852397  0.090201    113.jpg\n",
      "18   0.138150  0.543903  0.317947    114.jpg\n",
      "19   0.005525  0.597465  0.397010    115.jpg\n",
      "20   0.047546  0.879903  0.072551    116.jpg\n",
      "21   0.815218  0.172672  0.012110    117.jpg\n",
      "22   0.006269  0.075611  0.918120    118.jpg\n",
      "23   0.332183  0.645603  0.022213    119.jpg\n",
      "24   0.009526  0.456177  0.534297     12.jpg\n",
      "25   0.152640  0.772114  0.075246    120.jpg\n",
      "26   0.008281  0.591512  0.400207    121.jpg\n",
      "27   0.010982  0.279552  0.709466    122.jpg\n",
      "28   0.197309  0.775378  0.027313    123.jpg\n",
      "29   0.527360  0.352186  0.120454    124.jpg\n",
      "30   0.026951  0.863890  0.109159    125.jpg\n",
      "31   0.018907  0.873816  0.107277    126.jpg\n",
      "32   0.087387  0.832437  0.080176    127.jpg\n",
      "33   0.272360  0.657448  0.070192    128.jpg\n",
      "34   0.165517  0.825980  0.008503    129.jpg\n",
      "35   0.046173  0.553415  0.400412     13.jpg\n",
      "36   0.055719  0.865984  0.078297    130.jpg\n",
      "37   0.006098  0.590715  0.403186    131.jpg\n",
      "38   0.049545  0.617446  0.333009    132.jpg\n",
      "39   0.016230  0.659133  0.324636    133.jpg\n",
      "40   0.078035  0.886006  0.035959    134.jpg\n",
      "41   0.137848  0.790347  0.071805    135.jpg\n",
      "42   0.026231  0.926423  0.047346    136.jpg\n",
      "43   0.215535  0.740067  0.044398    137.jpg\n",
      "44   0.150470  0.729319  0.120211    138.jpg\n",
      "45   0.121409  0.849515  0.029076    139.jpg\n",
      "46   0.073674  0.824140  0.102186     14.jpg\n",
      "47   0.034758  0.670644  0.294597    140.jpg\n",
      "48   0.070586  0.857888  0.071526    141.jpg\n",
      "49   0.911350  0.085235  0.003415    142.jpg\n",
      "50   0.101073  0.519322  0.379605    143.jpg\n",
      "51   0.574501  0.322686  0.102813    144.jpg\n",
      "52   0.063991  0.837428  0.098581    145.jpg\n",
      "53   0.177416  0.710340  0.112245    146.jpg\n",
      "54   0.023877  0.472621  0.503502    147.jpg\n",
      "55   0.289575  0.690836  0.019589    148.jpg\n",
      "56   0.016892  0.968641  0.014467    149.jpg\n",
      "57   0.263622  0.247061  0.489317     15.jpg\n",
      "58   0.092990  0.848642  0.058368    150.jpg\n",
      "59   0.504238  0.169070  0.326692    151.jpg\n",
      "60   0.032596  0.452991  0.514413    152.jpg\n",
      "61   0.242397  0.479072  0.278531    153.jpg\n",
      "62   0.009076  0.863913  0.127011    155.jpg\n",
      "63   0.014478  0.083132  0.902390    156.jpg\n",
      "64   0.056666  0.927900  0.015433    157.jpg\n",
      "65   0.093960  0.796897  0.109143    158.jpg\n",
      "66   0.405988  0.412100  0.181912    159.jpg\n",
      "67   0.089420  0.561278  0.349301     16.jpg\n",
      "68   0.035684  0.414222  0.550094    161.jpg\n",
      "69   0.140933  0.744579  0.114488    162.jpg\n",
      "70   0.081913  0.881719  0.036368    163.jpg\n",
      "71   0.097355  0.785114  0.117531    164.jpg\n",
      "72   0.558441  0.432056  0.009503    165.jpg\n",
      "73   0.463425  0.506865  0.029710    166.jpg\n",
      "74   0.117063  0.342302  0.540634    167.jpg\n",
      "75   0.247099  0.742759  0.010141    168.jpg\n",
      "76   0.072547  0.872035  0.055418    169.jpg\n",
      "77   0.168921  0.815823  0.015257     17.jpg\n",
      "78   0.464341  0.499455  0.036204    170.jpg\n",
      "79   0.015539  0.468096  0.516365    171.jpg\n",
      "80   0.353967  0.159856  0.486177    172.jpg\n",
      "81   0.007427  0.123714  0.868859    173.jpg\n",
      "82   0.699551  0.044950  0.255499    174.jpg\n",
      "83   0.547909  0.153453  0.298638    175.jpg\n",
      "84   0.025373  0.552810  0.421817    176.jpg\n",
      "85   0.687841  0.090346  0.221813    177.jpg\n",
      "86   0.109882  0.737143  0.152975    178.jpg\n",
      "87   0.553342  0.435183  0.011475    179.jpg\n",
      "88   0.023329  0.824655  0.152015     18.jpg\n",
      "89   0.116682  0.616080  0.267237    180.jpg\n",
      "90   0.010454  0.758616  0.230930    181.jpg\n",
      "91   0.008400  0.031097  0.960503    182.jpg\n",
      "92   0.040287  0.780412  0.179301    183.jpg\n",
      "93   0.003946  0.228320  0.767733    184.jpg\n",
      "94   0.025559  0.894102  0.080339    185.jpg\n",
      "95   0.383877  0.581356  0.034766    186.jpg\n",
      "96   0.005356  0.366940  0.627705    187.jpg\n",
      "97   0.218891  0.649067  0.132042    188.jpg\n",
      "98   0.200276  0.644896  0.154828    189.jpg\n",
      "99   0.051839  0.849266  0.098895     19.jpg\n",
      "100  0.016688  0.282365  0.700947    190.jpg\n",
      "101  0.078980  0.870564  0.050456    191.jpg\n",
      "102  0.019191  0.711096  0.269713    192.jpg\n",
      "103  0.132033  0.841759  0.026208    193.jpg\n",
      "104  0.019302  0.425431  0.555267    194.jpg\n",
      "105  0.590221  0.334672  0.075108    195.jpg\n",
      "106  0.183438  0.780017  0.036545    197.jpg\n",
      "107  0.342882  0.315172  0.341946    198.jpg\n",
      "108  0.228578  0.428355  0.343066    199.jpg\n",
      "109  0.058500  0.904181  0.037319      2.jpg\n",
      "110  0.247515  0.740578  0.011907     20.jpg\n",
      "111  0.023240  0.246425  0.730335    200.jpg\n",
      "112  0.268053  0.696621  0.035326    201.jpg\n",
      "113  0.018426  0.298312  0.683262    202.jpg\n",
      "114  0.323093  0.610519  0.066388    203.jpg\n",
      "115  0.021195  0.903580  0.075225    204.jpg\n",
      "116  0.024619  0.382143  0.593238    205.jpg\n",
      "117  0.030491  0.702638  0.266872    206.jpg\n",
      "118  0.015650  0.762399  0.221951    207.jpg\n",
      "119  0.011571  0.612068  0.376362    208.jpg\n",
      "120  0.013671  0.175385  0.810943    209.jpg\n",
      "121  0.316721  0.511714  0.171565     21.jpg\n",
      "122  0.413820  0.171219  0.414961    210.jpg\n",
      "123  0.002743  0.029137  0.968120    211.jpg\n",
      "124  0.149604  0.369547  0.480848    212.jpg\n",
      "125  0.053473  0.634884  0.311643    213.jpg\n",
      "126  0.211361  0.774262  0.014377    214.jpg\n",
      "127  0.142798  0.848086  0.009116    215.jpg\n",
      "128  0.180954  0.796359  0.022687    216.jpg\n",
      "129  0.797724  0.197199  0.005078    217.jpg\n",
      "130  0.077174  0.359311  0.563515    218.jpg\n",
      "131  0.140640  0.710172  0.149188    219.jpg\n",
      "132  0.344162  0.388889  0.266949     22.jpg\n",
      "133  0.017636  0.764344  0.218020    220.jpg\n",
      "134  0.326966  0.495721  0.177313    221.jpg\n",
      "135  0.157505  0.705088  0.137406    222.jpg\n",
      "136  0.060007  0.064176  0.875817    223.jpg\n",
      "137  0.077185  0.568658  0.354157    224.jpg\n",
      "138  0.084294  0.719781  0.195925    225.jpg\n",
      "139  0.445893  0.478685  0.075422    226.jpg\n",
      "140  0.308151  0.527287  0.164563    227.jpg\n",
      "141  0.588848  0.403959  0.007194    228.jpg\n",
      "142  0.020256  0.734964  0.244779    229.jpg\n",
      "143  0.173412  0.295865  0.530723     23.jpg\n",
      "144  0.081465  0.778525  0.140010    230.jpg\n",
      "145  0.869869  0.126836  0.003295    231.jpg\n",
      "146  0.261223  0.726408  0.012369    232.jpg\n",
      "147  0.577054  0.333124  0.089821    233.jpg\n",
      "148  0.044106  0.523956  0.431938    234.jpg\n",
      "149  0.023521  0.925059  0.051421    235.jpg\n",
      "150  0.011009  0.484315  0.504675    236.jpg\n",
      "151  0.004983  0.928397  0.066620    237.jpg\n",
      "152  0.023999  0.244376  0.731625    238.jpg\n",
      "153  0.082550  0.884138  0.033312    239.jpg\n",
      "154  0.365930  0.421096  0.212974     24.jpg\n",
      "155  0.017673  0.087044  0.895283    240.jpg\n",
      "156  0.069219  0.727631  0.203150    241.jpg\n",
      "157  0.069175  0.773862  0.156962    242.jpg\n",
      "158  0.014428  0.841295  0.144278    244.jpg\n",
      "159  0.296979  0.683050  0.019971    245.jpg\n",
      "160  0.113776  0.783964  0.102260    246.jpg\n",
      "161  0.105766  0.184413  0.709821    247.jpg\n",
      "162  0.030064  0.760069  0.209867    248.jpg\n",
      "163  0.041335  0.250233  0.708432    249.jpg\n",
      "164  0.015199  0.907262  0.077539     25.jpg\n",
      "165  0.009629  0.660445  0.329926    250.jpg\n",
      "166  0.247106  0.307986  0.444909    251.jpg\n",
      "167  0.007302  0.493193  0.499505    252.jpg\n",
      "168  0.040328  0.590821  0.368851    253.jpg\n",
      "169  0.031319  0.306103  0.662577    254.jpg\n",
      "170  0.049350  0.805923  0.144726    255.jpg\n",
      "171  0.010235  0.642553  0.347212    256.jpg\n",
      "172  0.066162  0.859811  0.074028    257.jpg\n",
      "173  0.016826  0.274010  0.709163    258.jpg\n",
      "174  0.032702  0.908528  0.058770    259.jpg\n",
      "175  0.005268  0.966228  0.028504     26.jpg\n",
      "176  0.128810  0.829484  0.041707    260.jpg\n",
      "177  0.043739  0.932099  0.024161    261.jpg\n",
      "178  0.123939  0.812585  0.063475    262.jpg\n",
      "179  0.077198  0.886588  0.036214    263.jpg\n",
      "180  0.343236  0.497319  0.159445    264.jpg\n",
      "181  0.139755  0.452067  0.408178    265.jpg\n",
      "182  0.021103  0.512782  0.466115    266.jpg\n",
      "183  0.593632  0.395811  0.010557    267.jpg\n",
      "184  0.239953  0.726901  0.033145    268.jpg\n",
      "185  0.442371  0.544674  0.012955    269.jpg\n",
      "186  0.004851  0.591922  0.403227     27.jpg\n",
      "187  0.046944  0.818535  0.134521    270.jpg\n",
      "188  0.036079  0.854852  0.109069    271.jpg\n",
      "189  0.029254  0.552507  0.418239    272.jpg\n",
      "190  0.082968  0.310514  0.606518    273.jpg\n",
      "191  0.012895  0.843304  0.143800    274.jpg\n",
      "192  0.148603  0.668470  0.182927    275.jpg\n",
      "193  0.057048  0.692374  0.250578    276.jpg\n",
      "194  0.019931  0.343960  0.636109    278.jpg\n",
      "195  0.126649  0.436427  0.436924    279.jpg\n",
      "196  0.480034  0.482323  0.037643     28.jpg\n",
      "197  0.598999  0.392018  0.008983    280.jpg\n",
      "198  0.080296  0.378614  0.541090    281.jpg\n",
      "199  0.443588  0.482521  0.073891    282.jpg\n",
      "200  0.007080  0.651753  0.341167    283.jpg\n",
      "201  0.421203  0.570574  0.008223    285.jpg\n",
      "202  0.129827  0.815347  0.054826    286.jpg\n",
      "203  0.051460  0.417616  0.530924    287.jpg\n",
      "204  0.112282  0.808804  0.078913    288.jpg\n",
      "205  0.242371  0.715052  0.042578    289.jpg\n",
      "206  0.039144  0.910850  0.050006     29.jpg\n",
      "207  0.023702  0.386682  0.589616    290.jpg\n",
      "208  0.267973  0.718941  0.013086    291.jpg\n",
      "209  0.007426  0.956459  0.036115    292.jpg\n",
      "210  0.446518  0.498085  0.055397    293.jpg\n",
      "211  0.079072  0.897240  0.023688    294.jpg\n",
      "212  0.056581  0.869631  0.073788    295.jpg\n",
      "213  0.083041  0.851729  0.065229    296.jpg\n",
      "214  0.112713  0.817270  0.070017    297.jpg\n",
      "215  0.034071  0.806344  0.159585    298.jpg\n",
      "216  0.015135  0.680715  0.304151    299.jpg\n",
      "217  0.045987  0.804422  0.149591      3.jpg\n",
      "218  0.064071  0.815916  0.120013     30.jpg\n",
      "219  0.060508  0.925710  0.013782    300.jpg\n",
      "220  0.010526  0.798707  0.190767    301.jpg\n",
      "221  0.075575  0.559338  0.365087    302.jpg\n",
      "222  0.805406  0.171914  0.022680    303.jpg\n",
      "223  0.962622  0.035258  0.002121    304.jpg\n",
      "224  0.017743  0.939472  0.042785    305.jpg\n",
      "225  0.012227  0.911642  0.076130    306.jpg\n",
      "226  0.029980  0.177774  0.792247    307.jpg\n",
      "227  0.124242  0.828182  0.047576    308.jpg\n",
      "228  0.023118  0.692922  0.283960    309.jpg\n",
      "229  0.007656  0.899222  0.093122     31.jpg\n",
      "230  0.155435  0.743488  0.101077    310.jpg\n",
      "231  0.004039  0.200377  0.795583    311.jpg\n",
      "232  0.008048  0.930487  0.061465    312.jpg\n",
      "233  0.062038  0.692988  0.244974    313.jpg\n",
      "234  0.529372  0.458081  0.012547    314.jpg\n",
      "235  0.161647  0.503262  0.335091    315.jpg\n",
      "236  0.327442  0.413666  0.258892    316.jpg\n",
      "237  0.103211  0.527809  0.368980    317.jpg\n",
      "238  0.601500  0.309759  0.088742    318.jpg\n",
      "239  0.381132  0.494641  0.124227    319.jpg\n",
      "240  0.077348  0.900575  0.022076     32.jpg\n",
      "241  0.058151  0.813492  0.128358    320.jpg\n",
      "242  0.131141  0.591033  0.277826    321.jpg\n",
      "243  0.116518  0.505451  0.378031    322.jpg\n",
      "244  0.025620  0.828933  0.145447    323.jpg\n",
      "245  0.009459  0.764819  0.225722    324.jpg\n",
      "246  0.063390  0.120361  0.816249    325.jpg\n",
      "247  0.016201  0.352136  0.631663    326.jpg\n",
      "248  0.077066  0.597868  0.325066    327.jpg\n",
      "249  0.064572  0.804379  0.131049    328.jpg\n",
      "250  0.071341  0.522902  0.405757    329.jpg\n",
      "251  0.254974  0.581081  0.163945     33.jpg\n",
      "252  0.050560  0.562980  0.386460    330.jpg\n",
      "253  0.288568  0.583439  0.127993    332.jpg\n",
      "254  0.258784  0.360658  0.380558    333.jpg\n",
      "255  0.027616  0.397286  0.575098    334.jpg\n",
      "256  0.085296  0.482769  0.431935    335.jpg\n",
      "257  0.177527  0.806579  0.015894    336.jpg\n",
      "258  0.164184  0.611793  0.224023    337.jpg\n",
      "259  0.228999  0.497596  0.273405    338.jpg\n",
      "260  0.013070  0.306675  0.680254    339.jpg\n",
      "261  0.062334  0.734885  0.202782     34.jpg\n",
      "262  0.627942  0.362413  0.009644    340.jpg\n",
      "263  0.017839  0.488032  0.494129    341.jpg\n",
      "264  0.037874  0.875639  0.086486    342.jpg\n",
      "265  0.004761  0.434970  0.560269    343.jpg\n",
      "266  0.075110  0.887566  0.037324    344.jpg\n",
      "267  0.217635  0.508640  0.273725    345.jpg\n",
      "268  0.015287  0.688544  0.296169    346.jpg\n",
      "269  0.679391  0.310188  0.010421    347.jpg\n",
      "270  0.008209  0.129470  0.862321    348.jpg\n",
      "271  0.052206  0.795332  0.152462    349.jpg\n",
      "272  0.021536  0.812691  0.165773     35.jpg\n",
      "273  0.004086  0.336583  0.659331    350.jpg\n",
      "274  0.129851  0.739351  0.130798    351.jpg\n",
      "275  0.033589  0.936448  0.029963    352.jpg\n",
      "276  0.027829  0.313641  0.658530    353.jpg\n",
      "277  0.032247  0.950640  0.017113    354.jpg\n",
      "278  0.860672  0.132568  0.006760    355.jpg\n",
      "279  0.770317  0.219712  0.009971    356.jpg\n",
      "280  0.149094  0.665439  0.185467    357.jpg\n",
      "281  0.103596  0.880772  0.015633    358.jpg\n",
      "282  0.023997  0.712630  0.263373    359.jpg\n",
      "283  0.213876  0.688799  0.097325     36.jpg\n",
      "284  0.195779  0.759819  0.044403    360.jpg\n",
      "285  0.006914  0.184478  0.808608    361.jpg\n",
      "286  0.021532  0.757315  0.221153    362.jpg\n",
      "287  0.041873  0.198176  0.759951    363.jpg\n",
      "288  0.032106  0.158975  0.808919    364.jpg\n",
      "289  0.030383  0.262467  0.707150    365.jpg\n",
      "290  0.144706  0.314368  0.540926    366.jpg\n",
      "291  0.137196  0.836758  0.026046    367.jpg\n",
      "292  0.045932  0.462945  0.491123    368.jpg\n",
      "293  0.039959  0.758366  0.201675    369.jpg\n",
      "294  0.353287  0.573776  0.072938     37.jpg\n",
      "295  0.230310  0.294681  0.475009    370.jpg\n",
      "296  0.104273  0.870876  0.024851    371.jpg\n",
      "297  0.356385  0.079229  0.564386    372.jpg\n",
      "298  0.243028  0.666294  0.090678    373.jpg\n",
      "299  0.383848  0.114256  0.501896    374.jpg\n",
      "300  0.035502  0.597155  0.367343    375.jpg\n",
      "301  0.008255  0.042532  0.949213    376.jpg\n",
      "302  0.068078  0.777435  0.154487    377.jpg\n",
      "303  0.061823  0.838717  0.099460    378.jpg\n",
      "304  0.012540  0.360842  0.626618    379.jpg\n",
      "305  0.039335  0.929033  0.031632     38.jpg\n",
      "306  0.020661  0.557322  0.422017    380.jpg\n",
      "307  0.082871  0.807979  0.109150    381.jpg\n",
      "308  0.416629  0.571012  0.012360    382.jpg\n",
      "309  0.256195  0.706528  0.037277    383.jpg\n",
      "310  0.009873  0.065606  0.924521    384.jpg\n",
      "311  0.019466  0.771918  0.208616    385.jpg\n",
      "312  0.056824  0.309837  0.633339    386.jpg\n",
      "313  0.037074  0.398463  0.564463    387.jpg\n",
      "314  0.074422  0.667352  0.258226    388.jpg\n",
      "315  0.168380  0.696231  0.135389    389.jpg\n",
      "316  0.090048  0.473162  0.436790     39.jpg\n",
      "317  0.029654  0.878516  0.091830    390.jpg\n",
      "318  0.015572  0.714268  0.270161    391.jpg\n",
      "319  0.298559  0.183352  0.518088    392.jpg\n",
      "320  0.043990  0.728076  0.227934    393.jpg\n",
      "321  0.042120  0.886806  0.071074    394.jpg\n",
      "322  0.022473  0.789953  0.187574    395.jpg\n",
      "323  0.033642  0.935031  0.031327    396.jpg\n",
      "324  0.750012  0.205307  0.044682    397.jpg\n",
      "325  0.007706  0.046140  0.946153    398.jpg\n",
      "326  0.026257  0.501390  0.472353    399.jpg\n",
      "327  0.003882  0.524594  0.471524      4.jpg\n",
      "328  0.026973  0.134652  0.838375     40.jpg\n",
      "329  0.094136  0.692981  0.212883    400.jpg\n",
      "330  0.005723  0.830520  0.163758    401.jpg\n",
      "331  0.278444  0.630632  0.090924    402.jpg\n",
      "332  0.426438  0.566645  0.006918    403.jpg\n",
      "333  0.241640  0.745763  0.012597    404.jpg\n",
      "334  0.565538  0.422899  0.011563    405.jpg\n",
      "335  0.028541  0.566252  0.405207    406.jpg\n",
      "336  0.021135  0.941890  0.036975    407.jpg\n",
      "337  0.016992  0.514625  0.468383    408.jpg\n",
      "338  0.028354  0.512987  0.458659    409.jpg\n",
      "339  0.177923  0.583536  0.238541     41.jpg\n",
      "340  0.014116  0.172005  0.813879    410.jpg\n",
      "341  0.064418  0.266217  0.669365    411.jpg\n",
      "342  0.031159  0.671916  0.296926    412.jpg\n",
      "343  0.518033  0.307290  0.174677    413.jpg\n",
      "344  0.093729  0.755219  0.151052    414.jpg\n",
      "345  0.822519  0.156885  0.020596    415.jpg\n",
      "346  0.061071  0.911614  0.027314    416.jpg\n",
      "347  0.117822  0.861447  0.020732    417.jpg\n",
      "348  0.114799  0.219170  0.666032    418.jpg\n",
      "349  0.054903  0.897689  0.047408    419.jpg\n",
      "350  0.494013  0.413650  0.092337     42.jpg\n",
      "351  0.008896  0.818441  0.172663    420.jpg\n",
      "352  0.008302  0.343806  0.647892    421.jpg\n",
      "353  0.105262  0.681456  0.213282    422.jpg\n",
      "354  0.037789  0.671653  0.290558    423.jpg\n",
      "355  0.007064  0.030509  0.962427    424.jpg\n",
      "356  0.010443  0.508676  0.480882    425.jpg\n",
      "357  0.130523  0.424785  0.444692    426.jpg\n",
      "358  0.108596  0.775903  0.115500    427.jpg\n",
      "359  0.030941  0.849623  0.119436    428.jpg\n",
      "360  0.005542  0.538226  0.456233    429.jpg\n",
      "361  0.023429  0.591489  0.385082     43.jpg\n",
      "362  0.018819  0.899438  0.081744    430.jpg\n",
      "363  0.120719  0.867262  0.012020    431.jpg\n",
      "364  0.029883  0.763569  0.206548    432.jpg\n",
      "365  0.060630  0.354627  0.584742    433.jpg\n",
      "366  0.055624  0.303709  0.640667    434.jpg\n",
      "367  0.002776  0.577469  0.419755    435.jpg\n",
      "368  0.048044  0.919938  0.032018    436.jpg\n",
      "369  0.043917  0.288890  0.667193    437.jpg\n",
      "370  0.070706  0.897682  0.031612    438.jpg\n",
      "371  0.184796  0.504630  0.310574    439.jpg\n",
      "372  0.014440  0.684060  0.301500     44.jpg\n",
      "373  0.016278  0.176905  0.806817    440.jpg\n",
      "374  0.083033  0.841532  0.075435    441.jpg\n",
      "375  0.080639  0.794903  0.124458    442.jpg\n",
      "376  0.068086  0.456031  0.475883    443.jpg\n",
      "377  0.363817  0.452943  0.183240    444.jpg\n",
      "378  0.047373  0.909878  0.042749    445.jpg\n",
      "379  0.008050  0.277348  0.714602    446.jpg\n",
      "380  0.009384  0.962911  0.027704    447.jpg\n",
      "381  0.029698  0.356253  0.614049    448.jpg\n",
      "382  0.068734  0.503118  0.428149    449.jpg\n",
      "383  0.064066  0.682857  0.253077     45.jpg\n",
      "384  0.115970  0.609243  0.274787    450.jpg\n",
      "385  0.055432  0.907458  0.037110    451.jpg\n",
      "386  0.010107  0.103300  0.886593    452.jpg\n",
      "387  0.294345  0.669069  0.036586    453.jpg\n",
      "388  0.012373  0.601389  0.386239    454.jpg\n",
      "389  0.204981  0.777178  0.017841    455.jpg\n",
      "390  0.018989  0.935381  0.045630    456.jpg\n",
      "391  0.684028  0.308827  0.007145    457.jpg\n",
      "392  0.053574  0.689743  0.256683    458.jpg\n",
      "393  0.285893  0.700195  0.013912    459.jpg\n",
      "394  0.031495  0.894624  0.073881     46.jpg\n",
      "395  0.010536  0.020484  0.968980    461.jpg\n",
      "396  0.153068  0.809956  0.036977    462.jpg\n",
      "397  0.469607  0.323833  0.206561    463.jpg\n",
      "398  0.107739  0.878638  0.013623    464.jpg\n",
      "399  0.173170  0.767640  0.059190    465.jpg\n",
      "400  0.306122  0.635700  0.058178    466.jpg\n",
      "401  0.113998  0.589535  0.296467    467.jpg\n",
      "402  0.030081  0.892156  0.077763    468.jpg\n",
      "403  0.014097  0.914300  0.071604    469.jpg\n",
      "404  0.136874  0.842866  0.020261     47.jpg\n",
      "405  0.054702  0.917237  0.028060    470.jpg\n",
      "406  0.446847  0.535576  0.017576    471.jpg\n",
      "407  0.165870  0.188253  0.645877    472.jpg\n",
      "408  0.029909  0.559190  0.410901    473.jpg\n",
      "409  0.017386  0.522789  0.459825    474.jpg\n",
      "410  0.103747  0.699277  0.196976    475.jpg\n",
      "411  0.036116  0.937865  0.026019    476.jpg\n",
      "412  0.492774  0.462783  0.044444    477.jpg\n",
      "413  0.013036  0.956961  0.030002    478.jpg\n",
      "414  0.081642  0.105673  0.812685    479.jpg\n",
      "415  0.009135  0.170913  0.819952     48.jpg\n",
      "416  0.021637  0.167059  0.811304    480.jpg\n",
      "417  0.019316  0.296731  0.683954    481.jpg\n",
      "418  0.032174  0.946947  0.020878    482.jpg\n",
      "419  0.082546  0.899005  0.018449    483.jpg\n",
      "420  0.355679  0.615597  0.028724    484.jpg\n",
      "421  0.024193  0.104476  0.871331    485.jpg\n",
      "422  0.019466  0.372002  0.608532    486.jpg\n",
      "423  0.298353  0.586847  0.114800    487.jpg\n",
      "424  0.128420  0.203826  0.667755    488.jpg\n",
      "425  0.056969  0.904158  0.038873    489.jpg\n",
      "426  0.022112  0.295450  0.682438     49.jpg\n",
      "427  0.022224  0.842341  0.135435    490.jpg\n",
      "428  0.018135  0.652943  0.328922    491.jpg\n",
      "429  0.295976  0.591996  0.112028    492.jpg\n",
      "430  0.184226  0.556906  0.258867    493.jpg\n",
      "431  0.050388  0.844170  0.105442    494.jpg\n",
      "432  0.039744  0.734624  0.225632    495.jpg\n",
      "433  0.032785  0.613749  0.353466    496.jpg\n",
      "434  0.150183  0.698361  0.151456    497.jpg\n",
      "435  0.029387  0.716275  0.254338    498.jpg\n",
      "436  0.303541  0.146113  0.550347    499.jpg\n",
      "437  0.013383  0.968502  0.018115      5.jpg\n",
      "438  0.021445  0.050351  0.928204     50.jpg\n",
      "439  0.465113  0.361556  0.173331    500.jpg\n",
      "440  0.238718  0.738832  0.022451    501.jpg\n",
      "441  0.045201  0.390883  0.563916    502.jpg\n",
      "442  0.079004  0.846049  0.074947    503.jpg\n",
      "443  0.050389  0.614657  0.334954    504.jpg\n",
      "444  0.308802  0.649173  0.042025    505.jpg\n",
      "445  0.174021  0.800000  0.025979    506.jpg\n",
      "446  0.022046  0.872759  0.105195    507.jpg\n",
      "447  0.012982  0.481149  0.505869    508.jpg\n",
      "448  0.125405  0.116429  0.758166    509.jpg\n",
      "449  0.874209  0.118014  0.007777     51.jpg\n",
      "450  0.008910  0.754637  0.236453    510.jpg\n",
      "451  0.339681  0.322899  0.337420    511.jpg\n",
      "452  0.032847  0.124213  0.842940     52.jpg\n",
      "453  0.005661  0.632784  0.361555     53.jpg\n",
      "454  0.008290  0.619989  0.371721     54.jpg\n",
      "455  0.261657  0.308692  0.429651     55.jpg\n",
      "456  0.387046  0.472086  0.140868     56.jpg\n",
      "457  0.043644  0.731387  0.224969     57.jpg\n",
      "458  0.049637  0.890262  0.060101     58.jpg\n",
      "459  0.903568  0.093447  0.002986     59.jpg\n",
      "460  0.246410  0.719719  0.033871      6.jpg\n",
      "461  0.030424  0.949957  0.019620     60.jpg\n",
      "462  0.168493  0.774144  0.057363     61.jpg\n",
      "463  0.096424  0.868081  0.035496     62.jpg\n",
      "464  0.050142  0.765786  0.184071     63.jpg\n",
      "465  0.847349  0.143336  0.009315     64.jpg\n",
      "466  0.866114  0.129855  0.004031     65.jpg\n",
      "467  0.193099  0.550894  0.256007     66.jpg\n",
      "468  0.020197  0.326825  0.652978     67.jpg\n",
      "469  0.076717  0.828564  0.094719     68.jpg\n",
      "470  0.048857  0.611091  0.340052     69.jpg\n",
      "471  0.093114  0.788489  0.118396      7.jpg\n",
      "472  0.025958  0.933390  0.040652     70.jpg\n",
      "473  0.617160  0.341026  0.041813     71.jpg\n",
      "474  0.146837  0.842975  0.010188     72.jpg\n",
      "475  0.041698  0.838311  0.119991     73.jpg\n",
      "476  0.191568  0.426946  0.381486     74.jpg\n",
      "477  0.358786  0.555970  0.085245     75.jpg\n",
      "478  0.099631  0.656478  0.243891     76.jpg\n",
      "479  0.007908  0.717136  0.274956     77.jpg\n",
      "480  0.033863  0.049101  0.917036     78.jpg\n",
      "481  0.444528  0.537215  0.018257     79.jpg\n",
      "482  0.029836  0.459508  0.510655      8.jpg\n",
      "483  0.518571  0.292165  0.189265     80.jpg\n",
      "484  0.298502  0.647078  0.054420     81.jpg\n",
      "485  0.460246  0.483705  0.056049     82.jpg\n",
      "486  0.360873  0.390069  0.249058     83.jpg\n",
      "487  0.172627  0.796129  0.031243     84.jpg\n",
      "488  0.243005  0.736989  0.020006     85.jpg\n",
      "489  0.079362  0.646098  0.274540     86.jpg\n",
      "490  0.336998  0.432520  0.230482     87.jpg\n",
      "491  0.207130  0.784204  0.008667     88.jpg\n",
      "492  0.137001  0.516446  0.346554     89.jpg\n",
      "493  0.880888  0.072206  0.046906      9.jpg\n",
      "494  0.110753  0.337675  0.551573     90.jpg\n",
      "495  0.330736  0.654186  0.015078     91.jpg\n",
      "496  0.117485  0.525003  0.357511     92.jpg\n",
      "497  0.038008  0.495247  0.466744     93.jpg\n",
      "498  0.288403  0.623959  0.087638     94.jpg\n",
      "499  0.014330  0.596038  0.389632     95.jpg\n",
      "500  0.015473  0.942261  0.042266     96.jpg\n",
      "501  0.070036  0.894437  0.035528     97.jpg\n",
      "502  0.050971  0.931791  0.017238     98.jpg\n",
      "503  0.034076  0.433858  0.532067     99.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    test(isTrue = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
