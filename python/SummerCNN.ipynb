{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFilter, ImageStat, Image, ImageDraw\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_multi(path):\n",
    "    #Input: file path of image\n",
    "    #Output: [path, {'size': size of image at path}]\n",
    "    try:\n",
    "        im_stats_im_ = Image.open(path)\n",
    "        return [path, {'size': im_stats_im_.size}]\n",
    "    except:\n",
    "        print(path)\n",
    "        return [path, {'size': [0,0]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def im_stats(im_stats_df): \n",
    "    #Input: dataframe of training images (type, image, path)\n",
    "    #Output: dataframe of training images (type, image, path, size)\n",
    "    im_stats_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(im_multi, im_stats_df['path'])\n",
    "    #p.map(f, [x, y, z]) returns a list [f[x], f[y], f[z]]\n",
    "    #For all paths in the inputted dataframe im_stats_df, they are passed through im_multi\n",
    "    #ret holds [[path, {'size': size of image at path}], ...]\n",
    "    for i in range(len(ret)):\n",
    "        im_stats_d[ret[i][0]] = ret[i][1]\n",
    "    im_stats_df['size'] = im_stats_df['path'].map(lambda x: ' '.join(str(s) for s in \n",
    "                                                                     im_stats_d[x]['size']))\n",
    "    #Adds additional column to original dataframe and formats size as 3264 4160\n",
    "    return im_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    #Input: file path of image\n",
    "    #Output: [original path, resized image]\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(img, (64, 64), cv2.INTER_LINEAR) #INTER_LINEAR is algorithm \n",
    "    #to downsize image\n",
    "    #I could try using INTER_AREA as, according to the URL below, could be better\n",
    "    #http://tanbakuchi.com/posts/comparison-of-openv-interpolation-algorithms/\n",
    "    return [path, resized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image_features(paths):\n",
    "    #Input: list of paths\n",
    "    #Output: list of resized images that have been transposed for Conv2d layer\n",
    "    imf_d = {}\n",
    "    p = Pool(cpu_count())\n",
    "    ret = p.map(get_im_cv2, paths)\n",
    "    #ret holds a list: [[image path, resized image], ...]\n",
    "    for i in range(len(ret)):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "        #imf_d[image path in ret] = resized image\n",
    "    ret = []\n",
    "    fdata = [imf_d[f] for f in paths]\n",
    "    #fdata holds a list: [resized image, ...]\n",
    "    fdata = np.array(fdata, dtype=np.uint8)\n",
    "    #fdata is now a numbpy array of ints\n",
    "    fdata = fdata.transpose((0, 3, 1, 2))\n",
    "    #Usually its (2, 0, 1) since it changes the image from (0, 1, 2)->(width, height, channel)\n",
    "    #to (2, 0, 1)->(channel, width, height) for the Conv2d layer, \n",
    "    #but since we have 4 dimensions, it gets bumped up one to (3, 1, 2);\n",
    "    #I don't get why it has 4 dimensions tho (https://skymind.ai/wiki/convolutional-network)\n",
    "    fdata = fdata.astype('float32')\n",
    "    fdata = fdata / 255\n",
    "    #fdata now has values between 0 and 1\n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in training set: % 1478\n",
      "loading train data\n",
      "train data loaded\n",
      "['Type_1' 'Type_2' 'Type_3']\n",
      "loading test data\n",
      "test data loaded\n"
     ]
    }
   ],
   "source": [
    "train = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/Train/**/*.jpg\")\n",
    "#train is an array holding all of the path files in the training set\n",
    "print(\"Number of files in training set: %\", len(train))\n",
    "train = pd.DataFrame([[p.split('/')[7],p.split('/')[8],p] for p in train], columns = \n",
    "                     ['type','image','path'])[::1]\n",
    "#train is a dataframe holding the type (ex. \"Type_1\"), image name (ex. \"0.jpg\"), and file path\n",
    "#(ex. \"/Users/keerat/dev/AOSResearch/resources/Train/Type_1/0.jpg\")\n",
    "train = im_stats(train)\n",
    "#train now has additional column with size (ex. 3264 4160)\n",
    "train = train[train['size'] != '0 0'].reset_index(drop=True) #corrupt images removed\n",
    "#train now has an additional column with the index reset to 0, 1, 2, 3... instead of 0, 5, 10..\n",
    "print(\"loading train data\")\n",
    "train_data = normalize_image_features(train['path'])\n",
    "#train_data holds a usable set of training images for the CNN\n",
    "print(\"train data loaded\")\n",
    "np.save('train.npy', train_data, allow_pickle=True, fix_imports=True)\n",
    "#train.npy is a file that has all of the image arrays in train_data\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_target = le.fit_transform(train['type'].values) \n",
    "#train_target holds type of each image in train\n",
    "#Type_1 = 0, Type_2 = 1, Type_3 = 2\n",
    "#For example, if the 40th image in train is Type_1, then train_target[40] = 0\n",
    "print(le.classes_)  \n",
    "np.save('train_target.npy', train_target, allow_pickle=True, fix_imports=True)\n",
    "#train_target.npy is a file that has all values of train_target\n",
    "\n",
    "test = glob.glob(\"/Users/keerat/dev/AOSResearch/resources/test/*.jpg\")\n",
    "#test is an array holding all of the path files in the test set\n",
    "test = pd.DataFrame([[p.split('/')[7],p] for p in test], columns = ['image','path']) [::1]\n",
    "#test is a dataframe holding the image name (ex. \"0.jpg\"), and file path\n",
    "#(ex. \"/Users/keerat/dev/AOSResearch/resources/test/0.jpg\")\n",
    "print(\"loading test data\")\n",
    "test_data = normalize_image_features(test['path'])\n",
    "#test_data holds a usable set of test images for the CNN\n",
    "np.save('test.npy', test_data, allow_pickle=True, fix_imports=True)\n",
    "#test.npy is a file that has all of the image arrays in test_data\n",
    "print(\"test data loaded\")\n",
    "test_id = test.image.values\n",
    "np.save('test_id.npy', test_id, allow_pickle=True, fix_imports=True)\n",
    "#test_id.npy is a file that has all of the image names (ex. '0.jpg) in test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.set_floatx('float32')\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train.npy')\n",
    "train_target = np.load('train_target.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(opt_='adamax'):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th', \n",
    "                            input_shape=(3, 64, 64))) \n",
    "    #Could try different input shape\n",
    "    \n",
    "    #Activation='relu' to discover nonlinear patterns of data\n",
    "    #dim_ordering = 'th' to match (0, 3, 1, 2) images were transposed to\n",
    "    #Four 3x3 filters\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n",
    "    model.add(Dropout(0.2))\n",
    "    #Sets a fraction of rate of input units to 0 to prevent overfitting\n",
    "    model.add(Flatten())\n",
    "    #Creates 1D feature vector for Dense layers\n",
    "    model.add(Dense(12, activation='tanh')) #Classifies\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation='softmax')) #Classifies\n",
    "\n",
    "    model.compile(optimizer=opt_, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    #Compiles all layers of model with optimizer, loss function, and metrics (to evaluate \n",
    "    #performance)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanImages():\n",
    "    datagen = ImageDataGenerator(rotation_range=0.3, zoom_range=0.3)\n",
    "    #rotation_range = random rotation of images up to 0.3 degrees\n",
    "    #zoom_range = random zoom of images up to a scale of 0.3\n",
    "    datagen.fit(train_data)\n",
    "    return datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitAndPredict(): #Runs data through model\n",
    "    print(\"cleaning images\")\n",
    "    datagen=cleanImages() #datagen now points to the parameters in cleanImages()\n",
    "    print(\"images cleaned\")\n",
    "    \n",
    "    model = create_model() #model holds CNN model\n",
    "    x_train,x_val_train,y_train,y_val_train = train_test_split(train_data,train_target,\n",
    "                                                               test_size=0.4, random_state=17)\n",
    "    #x_train = training set of images (60% of original training set)\n",
    "    #x_val_train = validation set of images (40 % of original training set)\n",
    "    #y_train = types for training images\n",
    "    #y_val_train = types for validation images\n",
    "    \n",
    "    #Training set is used to fit the parameters using back prop\n",
    "    #Validation set is used to fine tune parameters to create a final model\n",
    "    #Test set is used to assess model's performance\n",
    "    print(\"fitting data\")\n",
    "    model.fit_generator(datagen.flow(x_train,y_train, batch_size=15, shuffle=True), \n",
    "                        nb_epoch=200, samples_per_epoch=len(x_train), \n",
    "                        verbose=2, validation_data=(x_val_train, y_val_train))\n",
    "    #Training set is augmented real-time with datagen.flow\n",
    "    #CNN processes images not one at a time, but in batches.  With batch_size = 15, one batch \n",
    "    #is 15 x 3 x 64 x 64.  The batch can't be too big, or else the machine can't handle it,\n",
    "    #and it can't be too small or else there will be no variance within the batch.\n",
    "    print(\"data fitted in model\")\n",
    "    test_data = np.load('test.npy')\n",
    "    test_id = np.load('test_id.npy')\n",
    "    print(\"creating predictions\")\n",
    "    predictions = model.predict_proba(test_data)\n",
    "    #Runs test_data through model and returns probablibity of it being each type\n",
    "    print(\"predictions made\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(isTrue): #Runs CNN on test images\n",
    "    pred=fitAndPredict()\n",
    "    print(\"creating test file\")\n",
    "    df = pd.DataFrame(pred, columns=['Type_1','Type_2','Type_3']) #Instantiates dataframe\n",
    "    df['image_name'] = test_id #image_name holds the .jpg file name\n",
    "    if (isTrue): #if(True), it will create a .csv file with the dataframe\n",
    "        df.to_csv('test.csv', index=False)\n",
    "        print(\"Test file created in users/keerat/...\")\n",
    "    else: #if(False), it will just show the dataframe\n",
    "        print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning images\n",
      "images cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", input_shape=(3, 64, 64..., data_format=\"channels_first\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", data_format=\"channels_first\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/Users/keerat/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=2, validation_data=(array([[[..., steps_per_epoch=59, epochs=200)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 7s - loss: 0.9973 - acc: 0.5355 - val_loss: 1.0534 - val_acc: 0.5203\n",
      "Epoch 2/200\n",
      " - 6s - loss: 1.0070 - acc: 0.5233 - val_loss: 1.0248 - val_acc: 0.5203\n",
      "Epoch 3/200\n",
      " - 6s - loss: 0.9879 - acc: 0.5323 - val_loss: 1.0350 - val_acc: 0.5203\n",
      "Epoch 4/200\n",
      " - 6s - loss: 0.9949 - acc: 0.5109 - val_loss: 1.0272 - val_acc: 0.5203\n",
      "Epoch 5/200\n",
      " - 6s - loss: 0.9713 - acc: 0.5411 - val_loss: 1.0236 - val_acc: 0.5203\n",
      "Epoch 6/200\n",
      " - 6s - loss: 0.9738 - acc: 0.5423 - val_loss: 1.0503 - val_acc: 0.5203\n",
      "Epoch 7/200\n",
      " - 7s - loss: 0.9639 - acc: 0.5346 - val_loss: 1.0257 - val_acc: 0.5118\n",
      "Epoch 8/200\n",
      " - 6s - loss: 0.9748 - acc: 0.5276 - val_loss: 1.0802 - val_acc: 0.5203\n",
      "Epoch 9/200\n",
      " - 6s - loss: 0.9778 - acc: 0.5323 - val_loss: 1.0459 - val_acc: 0.5203\n",
      "Epoch 10/200\n",
      " - 6s - loss: 0.9621 - acc: 0.5558 - val_loss: 1.0295 - val_acc: 0.5051\n",
      "Epoch 11/200\n",
      " - 6s - loss: 0.9589 - acc: 0.5456 - val_loss: 1.0386 - val_acc: 0.5101\n",
      "Epoch 12/200\n",
      " - 7s - loss: 0.9615 - acc: 0.5435 - val_loss: 1.0253 - val_acc: 0.5051\n",
      "Epoch 13/200\n",
      " - 6s - loss: 0.9360 - acc: 0.5693 - val_loss: 1.0176 - val_acc: 0.5186\n",
      "Epoch 14/200\n",
      " - 6s - loss: 0.9607 - acc: 0.5335 - val_loss: 1.0231 - val_acc: 0.5034\n",
      "Epoch 15/200\n",
      " - 6s - loss: 0.9414 - acc: 0.5592 - val_loss: 1.0131 - val_acc: 0.4983\n",
      "Epoch 16/200\n",
      " - 7s - loss: 0.9756 - acc: 0.5335 - val_loss: 1.0123 - val_acc: 0.5152\n",
      "Epoch 17/200\n",
      " - 6s - loss: 0.9040 - acc: 0.5603 - val_loss: 0.9846 - val_acc: 0.5169\n",
      "Epoch 18/200\n",
      " - 6s - loss: 0.9002 - acc: 0.5730 - val_loss: 0.9689 - val_acc: 0.5101\n",
      "Epoch 19/200\n",
      " - 7s - loss: 0.9017 - acc: 0.5685 - val_loss: 0.9677 - val_acc: 0.5084\n",
      "Epoch 20/200\n",
      " - 7s - loss: 0.8739 - acc: 0.5989 - val_loss: 0.9767 - val_acc: 0.5135\n",
      "Epoch 21/200\n",
      " - 7s - loss: 0.9133 - acc: 0.5548 - val_loss: 0.9567 - val_acc: 0.5253\n",
      "Epoch 22/200\n",
      " - 6s - loss: 0.8513 - acc: 0.6033 - val_loss: 0.9591 - val_acc: 0.5203\n",
      "Epoch 23/200\n",
      " - 6s - loss: 0.8569 - acc: 0.6089 - val_loss: 0.9583 - val_acc: 0.5270\n",
      "Epoch 24/200\n",
      " - 6s - loss: 0.8872 - acc: 0.5863 - val_loss: 0.9652 - val_acc: 0.5203\n",
      "Epoch 25/200\n",
      " - 6s - loss: 0.8745 - acc: 0.5808 - val_loss: 0.9425 - val_acc: 0.5338\n",
      "Epoch 26/200\n",
      " - 6s - loss: 0.8460 - acc: 0.6067 - val_loss: 0.9761 - val_acc: 0.5287\n",
      "Epoch 27/200\n",
      " - 6s - loss: 0.8505 - acc: 0.6236 - val_loss: 0.9811 - val_acc: 0.5203\n",
      "Epoch 28/200\n",
      " - 6s - loss: 0.8424 - acc: 0.6088 - val_loss: 0.9741 - val_acc: 0.5220\n",
      "Epoch 29/200\n",
      " - 6s - loss: 0.8702 - acc: 0.6089 - val_loss: 0.9701 - val_acc: 0.5236\n",
      "Epoch 30/200\n",
      " - 6s - loss: 0.8634 - acc: 0.6168 - val_loss: 0.9580 - val_acc: 0.5287\n",
      "Epoch 31/200\n",
      " - 6s - loss: 0.8692 - acc: 0.5945 - val_loss: 0.9483 - val_acc: 0.5253\n",
      "Epoch 32/200\n",
      " - 6s - loss: 0.8404 - acc: 0.6281 - val_loss: 0.9481 - val_acc: 0.5186\n",
      "Epoch 33/200\n",
      " - 6s - loss: 0.8434 - acc: 0.6181 - val_loss: 0.9499 - val_acc: 0.5169\n",
      "Epoch 34/200\n",
      " - 7s - loss: 0.8169 - acc: 0.6306 - val_loss: 0.9401 - val_acc: 0.5287\n",
      "Epoch 35/200\n",
      " - 7s - loss: 0.8587 - acc: 0.5855 - val_loss: 0.9384 - val_acc: 0.5186\n",
      "Epoch 36/200\n",
      " - 6s - loss: 0.8399 - acc: 0.6168 - val_loss: 0.9384 - val_acc: 0.5253\n",
      "Epoch 37/200\n",
      " - 6s - loss: 0.8199 - acc: 0.6169 - val_loss: 0.9300 - val_acc: 0.5389\n",
      "Epoch 38/200\n",
      " - 6s - loss: 0.8126 - acc: 0.6159 - val_loss: 0.9512 - val_acc: 0.5287\n",
      "Epoch 39/200\n",
      " - 6s - loss: 0.8522 - acc: 0.5910 - val_loss: 0.9358 - val_acc: 0.5338\n",
      "Epoch 40/200\n",
      " - 6s - loss: 0.8042 - acc: 0.6247 - val_loss: 0.9626 - val_acc: 0.5304\n",
      "Epoch 41/200\n",
      " - 6s - loss: 0.8379 - acc: 0.6113 - val_loss: 0.9272 - val_acc: 0.5422\n",
      "Epoch 42/200\n",
      " - 6s - loss: 0.8352 - acc: 0.6326 - val_loss: 0.9359 - val_acc: 0.5422\n",
      "Epoch 43/200\n",
      " - 6s - loss: 0.8137 - acc: 0.6248 - val_loss: 0.9587 - val_acc: 0.5321\n",
      "Epoch 44/200\n",
      " - 6s - loss: 0.8586 - acc: 0.5955 - val_loss: 0.9505 - val_acc: 0.5270\n",
      "Epoch 45/200\n",
      " - 6s - loss: 0.7883 - acc: 0.6349 - val_loss: 0.9401 - val_acc: 0.5355\n",
      "Epoch 46/200\n",
      " - 7s - loss: 0.8309 - acc: 0.5979 - val_loss: 0.9599 - val_acc: 0.5236\n",
      "Epoch 47/200\n",
      " - 6s - loss: 0.8177 - acc: 0.6024 - val_loss: 0.9507 - val_acc: 0.5287\n",
      "Epoch 48/200\n",
      " - 6s - loss: 0.8102 - acc: 0.6383 - val_loss: 0.9505 - val_acc: 0.5439\n",
      "Epoch 49/200\n",
      " - 6s - loss: 0.8132 - acc: 0.6194 - val_loss: 0.9468 - val_acc: 0.5304\n",
      "Epoch 50/200\n",
      " - 6s - loss: 0.8155 - acc: 0.6058 - val_loss: 0.9358 - val_acc: 0.5287\n",
      "Epoch 51/200\n",
      " - 7s - loss: 0.8219 - acc: 0.6024 - val_loss: 0.9473 - val_acc: 0.5304\n",
      "Epoch 52/200\n",
      " - 7s - loss: 0.8076 - acc: 0.6327 - val_loss: 0.9303 - val_acc: 0.5422\n",
      "Epoch 53/200\n",
      " - 6s - loss: 0.8002 - acc: 0.6474 - val_loss: 0.9332 - val_acc: 0.5338\n",
      "Epoch 54/200\n",
      " - 7s - loss: 0.7946 - acc: 0.6496 - val_loss: 0.9412 - val_acc: 0.5422\n",
      "Epoch 55/200\n",
      " - 7s - loss: 0.8142 - acc: 0.6236 - val_loss: 0.9339 - val_acc: 0.5456\n",
      "Epoch 56/200\n",
      " - 7s - loss: 0.8232 - acc: 0.6261 - val_loss: 0.9383 - val_acc: 0.5253\n",
      "Epoch 57/200\n",
      " - 7s - loss: 0.7946 - acc: 0.6497 - val_loss: 0.9594 - val_acc: 0.5270\n",
      "Epoch 58/200\n",
      " - 7s - loss: 0.7858 - acc: 0.6519 - val_loss: 0.9275 - val_acc: 0.5557\n",
      "Epoch 59/200\n",
      " - 7s - loss: 0.7855 - acc: 0.6519 - val_loss: 0.9516 - val_acc: 0.5321\n",
      "Epoch 60/200\n",
      " - 6s - loss: 0.8031 - acc: 0.6395 - val_loss: 0.9634 - val_acc: 0.5338\n",
      "Epoch 61/200\n",
      " - 6s - loss: 0.7865 - acc: 0.6519 - val_loss: 0.9491 - val_acc: 0.5321\n",
      "Epoch 62/200\n",
      " - 6s - loss: 0.7947 - acc: 0.6428 - val_loss: 0.9510 - val_acc: 0.5490\n",
      "Epoch 63/200\n",
      " - 6s - loss: 0.8035 - acc: 0.6395 - val_loss: 0.9369 - val_acc: 0.5473\n",
      "Epoch 64/200\n",
      " - 6s - loss: 0.7767 - acc: 0.6621 - val_loss: 0.9568 - val_acc: 0.5338\n",
      "Epoch 65/200\n",
      " - 6s - loss: 0.7852 - acc: 0.6349 - val_loss: 0.9503 - val_acc: 0.5355\n",
      "Epoch 66/200\n",
      " - 8s - loss: 0.7715 - acc: 0.6541 - val_loss: 0.9815 - val_acc: 0.5389\n",
      "Epoch 67/200\n",
      " - 6s - loss: 0.8112 - acc: 0.6372 - val_loss: 0.9652 - val_acc: 0.5287\n",
      "Epoch 68/200\n",
      " - 6s - loss: 0.7900 - acc: 0.6307 - val_loss: 0.9513 - val_acc: 0.5321\n",
      "Epoch 69/200\n",
      " - 6s - loss: 0.7549 - acc: 0.6654 - val_loss: 0.9717 - val_acc: 0.5422\n",
      "Epoch 70/200\n",
      " - 7s - loss: 0.8049 - acc: 0.6499 - val_loss: 0.9835 - val_acc: 0.5524\n",
      "Epoch 71/200\n",
      " - 6s - loss: 0.7936 - acc: 0.6496 - val_loss: 0.9633 - val_acc: 0.5253\n",
      "Epoch 72/200\n",
      " - 6s - loss: 0.7865 - acc: 0.6417 - val_loss: 0.9546 - val_acc: 0.5405\n",
      "Epoch 73/200\n",
      " - 6s - loss: 0.7782 - acc: 0.6375 - val_loss: 0.9783 - val_acc: 0.5338\n",
      "Epoch 74/200\n",
      " - 8s - loss: 0.7534 - acc: 0.6465 - val_loss: 0.9763 - val_acc: 0.5304\n",
      "Epoch 75/200\n",
      " - 1417s - loss: 0.7825 - acc: 0.6553 - val_loss: 0.9522 - val_acc: 0.5473\n",
      "Epoch 76/200\n",
      " - 11s - loss: 0.7384 - acc: 0.6520 - val_loss: 0.9657 - val_acc: 0.5203\n",
      "Epoch 77/200\n",
      " - 9s - loss: 0.7986 - acc: 0.6150 - val_loss: 0.9749 - val_acc: 0.5507\n",
      "Epoch 78/200\n",
      " - 11s - loss: 0.7678 - acc: 0.6442 - val_loss: 0.9449 - val_acc: 0.5507\n",
      "Epoch 79/200\n",
      " - 12s - loss: 0.7267 - acc: 0.6892 - val_loss: 0.9698 - val_acc: 0.5507\n",
      "Epoch 80/200\n",
      " - 10s - loss: 0.7606 - acc: 0.6599 - val_loss: 0.9714 - val_acc: 0.5541\n",
      "Epoch 81/200\n",
      " - 16s - loss: 0.7927 - acc: 0.6207 - val_loss: 0.9865 - val_acc: 0.5405\n",
      "Epoch 82/200\n",
      " - 13s - loss: 0.7665 - acc: 0.6465 - val_loss: 0.9538 - val_acc: 0.5405\n",
      "Epoch 83/200\n",
      " - 9s - loss: 0.7879 - acc: 0.6375 - val_loss: 0.9455 - val_acc: 0.5456\n",
      "Epoch 84/200\n",
      " - 8s - loss: 0.7811 - acc: 0.6395 - val_loss: 0.9568 - val_acc: 0.5422\n",
      "Epoch 85/200\n",
      " - 7s - loss: 0.7543 - acc: 0.6609 - val_loss: 0.9486 - val_acc: 0.5389\n",
      "Epoch 86/200\n",
      " - 7s - loss: 0.7488 - acc: 0.6790 - val_loss: 0.9663 - val_acc: 0.5473\n",
      "Epoch 87/200\n",
      " - 7s - loss: 0.7410 - acc: 0.6847 - val_loss: 0.9581 - val_acc: 0.5507\n",
      "Epoch 88/200\n",
      " - 7s - loss: 0.7277 - acc: 0.6722 - val_loss: 0.9758 - val_acc: 0.5507\n",
      "Epoch 89/200\n",
      " - 6s - loss: 0.7529 - acc: 0.6420 - val_loss: 0.9698 - val_acc: 0.5676\n",
      "Epoch 90/200\n",
      " - 7s - loss: 0.7795 - acc: 0.6533 - val_loss: 0.9638 - val_acc: 0.5422\n",
      "Epoch 91/200\n",
      " - 6s - loss: 0.7354 - acc: 0.6711 - val_loss: 0.9703 - val_acc: 0.5355\n",
      "Epoch 92/200\n",
      " - 8s - loss: 0.7268 - acc: 0.6745 - val_loss: 0.9838 - val_acc: 0.5405\n",
      "Epoch 93/200\n",
      " - 8s - loss: 0.7581 - acc: 0.6632 - val_loss: 0.9836 - val_acc: 0.5355\n",
      "Epoch 94/200\n",
      " - 7s - loss: 0.7488 - acc: 0.6519 - val_loss: 0.9659 - val_acc: 0.5355\n",
      "Epoch 95/200\n",
      " - 7s - loss: 0.7229 - acc: 0.6802 - val_loss: 0.9800 - val_acc: 0.5473\n",
      "Epoch 96/200\n",
      " - 6s - loss: 0.7404 - acc: 0.6609 - val_loss: 0.9937 - val_acc: 0.5152\n",
      "Epoch 97/200\n",
      " - 6s - loss: 0.7417 - acc: 0.6835 - val_loss: 0.9923 - val_acc: 0.5287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      " - 6s - loss: 0.7640 - acc: 0.6688 - val_loss: 0.9787 - val_acc: 0.5338\n",
      "Epoch 99/200\n",
      " - 6s - loss: 0.7391 - acc: 0.6668 - val_loss: 0.9876 - val_acc: 0.5355\n",
      "Epoch 100/200\n",
      " - 6s - loss: 0.7317 - acc: 0.6803 - val_loss: 0.9744 - val_acc: 0.5372\n",
      "Epoch 101/200\n",
      " - 7s - loss: 0.6973 - acc: 0.6748 - val_loss: 0.9930 - val_acc: 0.5338\n",
      "Epoch 102/200\n",
      " - 6s - loss: 0.7534 - acc: 0.6576 - val_loss: 0.9796 - val_acc: 0.5507\n",
      "Epoch 103/200\n",
      " - 6s - loss: 0.7888 - acc: 0.6442 - val_loss: 0.9745 - val_acc: 0.5456\n",
      "Epoch 104/200\n",
      " - 7s - loss: 0.7112 - acc: 0.6995 - val_loss: 0.9768 - val_acc: 0.5557\n",
      "Epoch 105/200\n",
      " - 8s - loss: 0.7224 - acc: 0.6938 - val_loss: 0.9691 - val_acc: 0.5389\n",
      "Epoch 106/200\n",
      " - 6s - loss: 0.7342 - acc: 0.6711 - val_loss: 1.0051 - val_acc: 0.5473\n",
      "Epoch 107/200\n",
      " - 6s - loss: 0.7057 - acc: 0.7107 - val_loss: 0.9814 - val_acc: 0.5372\n",
      "Epoch 108/200\n",
      " - 6s - loss: 0.6983 - acc: 0.6892 - val_loss: 0.9856 - val_acc: 0.5389\n",
      "Epoch 109/200\n",
      " - 6s - loss: 0.7305 - acc: 0.6755 - val_loss: 0.9832 - val_acc: 0.5541\n",
      "Epoch 110/200\n",
      " - 6s - loss: 0.7145 - acc: 0.6746 - val_loss: 0.9821 - val_acc: 0.5574\n",
      "Epoch 111/200\n",
      " - 6s - loss: 0.7468 - acc: 0.6793 - val_loss: 0.9745 - val_acc: 0.5490\n",
      "Epoch 112/200\n",
      " - 6s - loss: 0.7111 - acc: 0.6847 - val_loss: 1.0043 - val_acc: 0.5473\n",
      "Epoch 113/200\n",
      " - 6s - loss: 0.6872 - acc: 0.6960 - val_loss: 0.9911 - val_acc: 0.5490\n",
      "Epoch 114/200\n",
      " - 6s - loss: 0.7106 - acc: 0.6971 - val_loss: 0.9939 - val_acc: 0.5473\n",
      "Epoch 115/200\n",
      " - 6s - loss: 0.7121 - acc: 0.6769 - val_loss: 0.9974 - val_acc: 0.5473\n",
      "Epoch 116/200\n",
      " - 6s - loss: 0.7582 - acc: 0.6678 - val_loss: 0.9819 - val_acc: 0.5389\n",
      "Epoch 117/200\n",
      " - 6s - loss: 0.6872 - acc: 0.6948 - val_loss: 0.9908 - val_acc: 0.5524\n",
      "Epoch 118/200\n",
      " - 6s - loss: 0.6912 - acc: 0.6858 - val_loss: 0.9894 - val_acc: 0.5287\n",
      "Epoch 119/200\n",
      " - 8s - loss: 0.7133 - acc: 0.6824 - val_loss: 1.0077 - val_acc: 0.5439\n",
      "Epoch 120/200\n",
      " - 6s - loss: 0.7096 - acc: 0.6917 - val_loss: 1.0025 - val_acc: 0.5321\n",
      "Epoch 121/200\n",
      " - 6s - loss: 0.7061 - acc: 0.6962 - val_loss: 0.9930 - val_acc: 0.5524\n",
      "Epoch 122/200\n",
      " - 6s - loss: 0.7063 - acc: 0.6722 - val_loss: 1.0048 - val_acc: 0.5422\n",
      "Epoch 123/200\n",
      " - 7s - loss: 0.6772 - acc: 0.7095 - val_loss: 0.9869 - val_acc: 0.5355\n",
      "Epoch 124/200\n",
      " - 6s - loss: 0.6979 - acc: 0.7027 - val_loss: 0.9931 - val_acc: 0.5355\n",
      "Epoch 125/200\n",
      " - 6s - loss: 0.6950 - acc: 0.6847 - val_loss: 1.0061 - val_acc: 0.5405\n",
      "Epoch 126/200\n",
      " - 6s - loss: 0.7117 - acc: 0.6960 - val_loss: 1.0114 - val_acc: 0.5490\n",
      "Epoch 127/200\n",
      " - 6s - loss: 0.6788 - acc: 0.6981 - val_loss: 1.0041 - val_acc: 0.5439\n",
      "Epoch 128/200\n",
      " - 6s - loss: 0.7036 - acc: 0.7050 - val_loss: 1.0125 - val_acc: 0.5389\n",
      "Epoch 129/200\n",
      " - 6s - loss: 0.6878 - acc: 0.6971 - val_loss: 1.0177 - val_acc: 0.5507\n",
      "Epoch 130/200\n",
      " - 6s - loss: 0.7037 - acc: 0.6903 - val_loss: 0.9935 - val_acc: 0.5422\n",
      "Epoch 131/200\n",
      " - 6s - loss: 0.6793 - acc: 0.6971 - val_loss: 1.0320 - val_acc: 0.5355\n",
      "Epoch 132/200\n",
      " - 6s - loss: 0.6889 - acc: 0.6847 - val_loss: 0.9980 - val_acc: 0.5372\n",
      "Epoch 133/200\n",
      " - 6s - loss: 0.6931 - acc: 0.6982 - val_loss: 1.0048 - val_acc: 0.5422\n",
      "Epoch 134/200\n",
      " - 6s - loss: 0.7151 - acc: 0.6861 - val_loss: 1.0303 - val_acc: 0.5220\n",
      "Epoch 135/200\n",
      " - 6s - loss: 0.6783 - acc: 0.7039 - val_loss: 1.0105 - val_acc: 0.5557\n",
      "Epoch 136/200\n",
      " - 6s - loss: 0.7124 - acc: 0.6926 - val_loss: 1.0442 - val_acc: 0.5389\n",
      "Epoch 137/200\n",
      " - 6s - loss: 0.7028 - acc: 0.6883 - val_loss: 1.0038 - val_acc: 0.5270\n",
      "Epoch 138/200\n",
      " - 6s - loss: 0.7015 - acc: 0.6870 - val_loss: 0.9948 - val_acc: 0.5422\n",
      "Epoch 139/200\n",
      " - 6s - loss: 0.7154 - acc: 0.6838 - val_loss: 1.0064 - val_acc: 0.5405\n",
      "Epoch 140/200\n",
      " - 6s - loss: 0.6828 - acc: 0.7018 - val_loss: 1.0231 - val_acc: 0.5422\n",
      "Epoch 141/200\n",
      " - 6s - loss: 0.6869 - acc: 0.7006 - val_loss: 1.0088 - val_acc: 0.5372\n",
      "Epoch 142/200\n",
      " - 6s - loss: 0.7000 - acc: 0.6917 - val_loss: 1.0400 - val_acc: 0.5389\n",
      "Epoch 143/200\n",
      " - 6s - loss: 0.6831 - acc: 0.6848 - val_loss: 1.0791 - val_acc: 0.5439\n",
      "Epoch 144/200\n",
      " - 6s - loss: 0.6706 - acc: 0.7129 - val_loss: 1.0300 - val_acc: 0.5405\n",
      "Epoch 145/200\n",
      " - 7s - loss: 0.6684 - acc: 0.7028 - val_loss: 1.0490 - val_acc: 0.5422\n",
      "Epoch 146/200\n",
      " - 6s - loss: 0.6701 - acc: 0.6994 - val_loss: 1.0502 - val_acc: 0.5473\n",
      "Epoch 147/200\n",
      " - 6s - loss: 0.6599 - acc: 0.7083 - val_loss: 1.0848 - val_acc: 0.5490\n",
      "Epoch 148/200\n",
      " - 8s - loss: 0.6830 - acc: 0.7242 - val_loss: 1.0457 - val_acc: 0.5490\n",
      "Epoch 149/200\n",
      " - 8s - loss: 0.6901 - acc: 0.6971 - val_loss: 1.0203 - val_acc: 0.5405\n",
      "Epoch 150/200\n",
      " - 6s - loss: 0.6708 - acc: 0.6996 - val_loss: 1.0222 - val_acc: 0.5557\n",
      "Epoch 151/200\n",
      " - 6s - loss: 0.6955 - acc: 0.6937 - val_loss: 1.0421 - val_acc: 0.5253\n",
      "Epoch 152/200\n",
      " - 6s - loss: 0.7515 - acc: 0.6691 - val_loss: 1.0157 - val_acc: 0.5355\n",
      "Epoch 153/200\n",
      " - 7s - loss: 0.6910 - acc: 0.6895 - val_loss: 1.0265 - val_acc: 0.5524\n",
      "Epoch 154/200\n",
      " - 6s - loss: 0.6761 - acc: 0.6914 - val_loss: 1.0306 - val_acc: 0.5507\n",
      "Epoch 155/200\n",
      " - 6s - loss: 0.6932 - acc: 0.7028 - val_loss: 1.0075 - val_acc: 0.5456\n",
      "Epoch 156/200\n",
      " - 6s - loss: 0.6282 - acc: 0.7468 - val_loss: 1.0192 - val_acc: 0.5422\n",
      "Epoch 157/200\n",
      " - 6s - loss: 0.6842 - acc: 0.6915 - val_loss: 1.0211 - val_acc: 0.5524\n",
      "Epoch 158/200\n",
      " - 7s - loss: 0.6473 - acc: 0.7208 - val_loss: 1.0388 - val_acc: 0.5253\n",
      "Epoch 159/200\n",
      " - 7s - loss: 0.6526 - acc: 0.7277 - val_loss: 1.0663 - val_acc: 0.5507\n",
      "Epoch 160/200\n",
      " - 6s - loss: 0.6814 - acc: 0.7027 - val_loss: 1.0442 - val_acc: 0.5507\n",
      "Epoch 161/200\n",
      " - 6s - loss: 0.6682 - acc: 0.6984 - val_loss: 1.0518 - val_acc: 0.5389\n",
      "Epoch 162/200\n",
      " - 6s - loss: 0.6810 - acc: 0.7096 - val_loss: 1.0244 - val_acc: 0.5541\n",
      "Epoch 163/200\n",
      " - 6s - loss: 0.6645 - acc: 0.7163 - val_loss: 1.0400 - val_acc: 0.5524\n",
      "Epoch 164/200\n",
      " - 6s - loss: 0.6280 - acc: 0.7445 - val_loss: 1.0591 - val_acc: 0.5405\n",
      "Epoch 165/200\n",
      " - 6s - loss: 0.6763 - acc: 0.7141 - val_loss: 1.0410 - val_acc: 0.5372\n",
      "Epoch 166/200\n",
      " - 7s - loss: 0.6565 - acc: 0.7242 - val_loss: 1.0457 - val_acc: 0.5253\n",
      "Epoch 167/200\n",
      " - 6s - loss: 0.6553 - acc: 0.7154 - val_loss: 1.0310 - val_acc: 0.5389\n",
      "Epoch 168/200\n",
      " - 7s - loss: 0.6572 - acc: 0.7129 - val_loss: 1.0503 - val_acc: 0.5389\n",
      "Epoch 169/200\n",
      " - 6s - loss: 0.6580 - acc: 0.7332 - val_loss: 1.0342 - val_acc: 0.5405\n",
      "Epoch 170/200\n",
      " - 6s - loss: 0.6377 - acc: 0.7311 - val_loss: 1.0319 - val_acc: 0.5220\n",
      "Epoch 171/200\n",
      " - 6s - loss: 0.6236 - acc: 0.7400 - val_loss: 1.0432 - val_acc: 0.5507\n",
      "Epoch 172/200\n",
      " - 6s - loss: 0.6602 - acc: 0.7199 - val_loss: 1.0473 - val_acc: 0.5355\n",
      "Epoch 173/200\n",
      " - 6s - loss: 0.6692 - acc: 0.7051 - val_loss: 1.0322 - val_acc: 0.5405\n",
      "Epoch 174/200\n",
      " - 6s - loss: 0.6166 - acc: 0.7299 - val_loss: 1.0740 - val_acc: 0.5439\n",
      "Epoch 175/200\n",
      " - 6s - loss: 0.6449 - acc: 0.7197 - val_loss: 1.0617 - val_acc: 0.5304\n",
      "Epoch 176/200\n",
      " - 6s - loss: 0.6805 - acc: 0.6793 - val_loss: 1.0460 - val_acc: 0.5304\n",
      "Epoch 177/200\n",
      " - 6s - loss: 0.6278 - acc: 0.7321 - val_loss: 1.0629 - val_acc: 0.5405\n",
      "Epoch 178/200\n",
      " - 6s - loss: 0.6559 - acc: 0.7098 - val_loss: 1.0483 - val_acc: 0.5439\n",
      "Epoch 179/200\n",
      " - 6s - loss: 0.6540 - acc: 0.7245 - val_loss: 1.0493 - val_acc: 0.5405\n",
      "Epoch 180/200\n",
      " - 6s - loss: 0.6643 - acc: 0.6940 - val_loss: 1.0666 - val_acc: 0.5355\n",
      "Epoch 181/200\n",
      " - 6s - loss: 0.6166 - acc: 0.7265 - val_loss: 1.0702 - val_acc: 0.5439\n",
      "Epoch 182/200\n",
      " - 6s - loss: 0.6377 - acc: 0.7265 - val_loss: 1.0581 - val_acc: 0.5372\n",
      "Epoch 183/200\n",
      " - 6s - loss: 0.6188 - acc: 0.7265 - val_loss: 1.0662 - val_acc: 0.5287\n",
      "Epoch 184/200\n",
      " - 7s - loss: 0.6182 - acc: 0.7536 - val_loss: 1.0670 - val_acc: 0.5287\n",
      "Epoch 185/200\n",
      " - 6s - loss: 0.6491 - acc: 0.7321 - val_loss: 1.0472 - val_acc: 0.5203\n",
      "Epoch 186/200\n",
      " - 6s - loss: 0.6391 - acc: 0.7310 - val_loss: 1.0612 - val_acc: 0.5287\n",
      "Epoch 187/200\n",
      " - 10s - loss: 0.6578 - acc: 0.7053 - val_loss: 1.0374 - val_acc: 0.5321\n",
      "Epoch 188/200\n",
      " - 15s - loss: 0.6358 - acc: 0.7321 - val_loss: 1.0736 - val_acc: 0.5389\n",
      "Epoch 189/200\n",
      " - 12s - loss: 0.6296 - acc: 0.7200 - val_loss: 1.0558 - val_acc: 0.5422\n",
      "Epoch 190/200\n",
      " - 12s - loss: 0.6361 - acc: 0.7140 - val_loss: 1.0607 - val_acc: 0.5287\n",
      "Epoch 191/200\n",
      " - 12s - loss: 0.6322 - acc: 0.7211 - val_loss: 1.0769 - val_acc: 0.5473\n",
      "Epoch 192/200\n",
      " - 10s - loss: 0.6399 - acc: 0.7008 - val_loss: 1.0600 - val_acc: 0.5135\n",
      "Epoch 193/200\n",
      " - 9s - loss: 0.6090 - acc: 0.7358 - val_loss: 1.0660 - val_acc: 0.5321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200\n",
      " - 8s - loss: 0.6151 - acc: 0.7367 - val_loss: 1.0709 - val_acc: 0.5456\n",
      "Epoch 195/200\n",
      " - 11s - loss: 0.6115 - acc: 0.7400 - val_loss: 1.0589 - val_acc: 0.5422\n",
      "Epoch 196/200\n",
      " - 10s - loss: 0.6186 - acc: 0.7245 - val_loss: 1.0444 - val_acc: 0.5270\n",
      "Epoch 197/200\n",
      " - 8s - loss: 0.6442 - acc: 0.7098 - val_loss: 1.0729 - val_acc: 0.5355\n",
      "Epoch 198/200\n",
      " - 8s - loss: 0.6701 - acc: 0.7234 - val_loss: 1.0591 - val_acc: 0.5304\n",
      "Epoch 199/200\n",
      " - 8s - loss: 0.6262 - acc: 0.7197 - val_loss: 1.0913 - val_acc: 0.5541\n",
      "Epoch 200/200\n",
      " - 8s - loss: 0.6581 - acc: 0.7121 - val_loss: 1.0539 - val_acc: 0.5270\n",
      "data fitted in model\n",
      "creating predictions\n",
      "predictions made\n",
      "creating test file\n",
      "       Type_1    Type_2    Type_3 image_name\n",
      "0    0.131986  0.450802  0.417212      0.jpg\n",
      "1    0.008711  0.029944  0.961346      1.jpg\n",
      "2    0.048231  0.136938  0.814831     10.jpg\n",
      "3    0.037731  0.560056  0.402213    100.jpg\n",
      "4    0.246215  0.055139  0.698646    101.jpg\n",
      "5    0.060885  0.055761  0.883354    102.jpg\n",
      "6    0.527436  0.192874  0.279689    103.jpg\n",
      "7    0.040817  0.312692  0.646491    104.jpg\n",
      "8    0.016546  0.382366  0.601088    105.jpg\n",
      "9    0.553282  0.204564  0.242154    106.jpg\n",
      "10   0.082435  0.744062  0.173503    107.jpg\n",
      "11   0.014384  0.700012  0.285605    108.jpg\n",
      "12   0.011748  0.338895  0.649357    109.jpg\n",
      "13   0.045385  0.344749  0.609866     11.jpg\n",
      "14   0.029373  0.846536  0.124090    110.jpg\n",
      "15   0.130647  0.666733  0.202620    111.jpg\n",
      "16   0.011296  0.161564  0.827140    112.jpg\n",
      "17   0.091565  0.732951  0.175483    113.jpg\n",
      "18   0.177691  0.326702  0.495607    114.jpg\n",
      "19   0.004663  0.599191  0.396146    115.jpg\n",
      "20   0.010614  0.882074  0.107312    116.jpg\n",
      "21   0.824578  0.163532  0.011890    117.jpg\n",
      "22   0.003465  0.110911  0.885624    118.jpg\n",
      "23   0.285709  0.682721  0.031570    119.jpg\n",
      "24   0.011830  0.199918  0.788253     12.jpg\n",
      "25   0.352084  0.548401  0.099515    120.jpg\n",
      "26   0.008511  0.724314  0.267175    121.jpg\n",
      "27   0.005696  0.118432  0.875873    122.jpg\n",
      "28   0.432072  0.544515  0.023414    123.jpg\n",
      "29   0.313214  0.508780  0.178005    124.jpg\n",
      "30   0.141879  0.298564  0.559557    125.jpg\n",
      "31   0.013320  0.810160  0.176521    126.jpg\n",
      "32   0.031482  0.803015  0.165504    127.jpg\n",
      "33   0.360967  0.588495  0.050537    128.jpg\n",
      "34   0.038530  0.921749  0.039721    129.jpg\n",
      "35   0.016132  0.695099  0.288768     13.jpg\n",
      "36   0.032805  0.893759  0.073436    130.jpg\n",
      "37   0.002342  0.359090  0.638567    131.jpg\n",
      "38   0.031737  0.611310  0.356953    132.jpg\n",
      "39   0.012394  0.410856  0.576750    133.jpg\n",
      "40   0.056907  0.916560  0.026533    134.jpg\n",
      "41   0.115808  0.692246  0.191946    135.jpg\n",
      "42   0.073770  0.860195  0.066035    136.jpg\n",
      "43   0.583881  0.288210  0.127908    137.jpg\n",
      "44   0.126632  0.581468  0.291900    138.jpg\n",
      "45   0.270070  0.672066  0.057864    139.jpg\n",
      "46   0.053579  0.667778  0.278643     14.jpg\n",
      "47   0.073090  0.558207  0.368703    140.jpg\n",
      "48   0.067911  0.858361  0.073727    141.jpg\n",
      "49   0.951861  0.045346  0.002793    142.jpg\n",
      "50   0.050056  0.575409  0.374535    143.jpg\n",
      "51   0.153762  0.775379  0.070859    144.jpg\n",
      "52   0.036000  0.834788  0.129212    145.jpg\n",
      "53   0.129660  0.592264  0.278076    146.jpg\n",
      "54   0.006400  0.335123  0.658477    147.jpg\n",
      "55   0.406202  0.565109  0.028688    148.jpg\n",
      "56   0.002646  0.975838  0.021516    149.jpg\n",
      "57   0.088576  0.160319  0.751105     15.jpg\n",
      "58   0.214305  0.729139  0.056556    150.jpg\n",
      "59   0.176058  0.525883  0.298058    151.jpg\n",
      "60   0.007593  0.279891  0.712516    152.jpg\n",
      "61   0.080255  0.472990  0.446755    153.jpg\n",
      "62   0.013180  0.889291  0.097529    155.jpg\n",
      "63   0.004754  0.049368  0.945878    156.jpg\n",
      "64   0.022815  0.949585  0.027600    157.jpg\n",
      "65   0.166756  0.451331  0.381913    158.jpg\n",
      "66   0.287865  0.468243  0.243893    159.jpg\n",
      "67   0.075263  0.529539  0.395198     16.jpg\n",
      "68   0.008373  0.384655  0.606972    161.jpg\n",
      "69   0.152956  0.648275  0.198769    162.jpg\n",
      "70   0.358892  0.606287  0.034822    163.jpg\n",
      "71   0.316908  0.413028  0.270064    164.jpg\n",
      "72   0.181032  0.802934  0.016035    165.jpg\n",
      "73   0.271379  0.699194  0.029427    166.jpg\n",
      "74   0.070686  0.431934  0.497381    167.jpg\n",
      "75   0.083856  0.893355  0.022789    168.jpg\n",
      "76   0.121348  0.845866  0.032786    169.jpg\n",
      "77   0.040210  0.947303  0.012486     17.jpg\n",
      "78   0.611704  0.356569  0.031727    170.jpg\n",
      "79   0.048901  0.467130  0.483968    171.jpg\n",
      "80   0.131334  0.425432  0.443234    172.jpg\n",
      "81   0.011346  0.131001  0.857652    173.jpg\n",
      "82   0.141137  0.112767  0.746096    174.jpg\n",
      "83   0.440738  0.422315  0.136946    175.jpg\n",
      "84   0.007825  0.270989  0.721186    176.jpg\n",
      "85   0.216161  0.196714  0.587125    177.jpg\n",
      "86   0.257557  0.548411  0.194032    178.jpg\n",
      "87   0.395327  0.592785  0.011888    179.jpg\n",
      "88   0.135956  0.812818  0.051227     18.jpg\n",
      "89   0.015192  0.317237  0.667571    180.jpg\n",
      "90   0.007226  0.213350  0.779424    181.jpg\n",
      "91   0.011943  0.043276  0.944781    182.jpg\n",
      "92   0.020996  0.892564  0.086440    183.jpg\n",
      "93   0.000944  0.167218  0.831838    184.jpg\n",
      "94   0.142170  0.697151  0.160680    185.jpg\n",
      "95   0.319198  0.549192  0.131610    186.jpg\n",
      "96   0.002155  0.256471  0.741373    187.jpg\n",
      "97   0.050313  0.490429  0.459257    188.jpg\n",
      "98   0.079233  0.889935  0.030831    189.jpg\n",
      "99   0.050350  0.709822  0.239828     19.jpg\n",
      "100  0.006958  0.449988  0.543053    190.jpg\n",
      "101  0.078631  0.872858  0.048511    191.jpg\n",
      "102  0.135387  0.740531  0.124082    192.jpg\n",
      "103  0.083295  0.891725  0.024981    193.jpg\n",
      "104  0.013450  0.782928  0.203621    194.jpg\n",
      "105  0.302446  0.483958  0.213596    195.jpg\n",
      "106  0.338965  0.634849  0.026186    197.jpg\n",
      "107  0.169121  0.423122  0.407757    198.jpg\n",
      "108  0.214718  0.575195  0.210087    199.jpg\n",
      "109  0.087438  0.865639  0.046923      2.jpg\n",
      "110  0.100100  0.890304  0.009596     20.jpg\n",
      "111  0.027723  0.315444  0.656833    200.jpg\n",
      "112  0.404087  0.516947  0.078966    201.jpg\n",
      "113  0.026574  0.443028  0.530398    202.jpg\n",
      "114  0.506715  0.336821  0.156464    203.jpg\n",
      "115  0.090747  0.791971  0.117282    204.jpg\n",
      "116  0.018131  0.302636  0.679233    205.jpg\n",
      "117  0.074258  0.836404  0.089338    206.jpg\n",
      "118  0.037785  0.620775  0.341441    207.jpg\n",
      "119  0.012833  0.303632  0.683535    208.jpg\n",
      "120  0.005857  0.235264  0.758878    209.jpg\n",
      "121  0.705146  0.215155  0.079699     21.jpg\n",
      "122  0.193168  0.408694  0.398138    210.jpg\n",
      "123  0.001896  0.039173  0.958932    211.jpg\n",
      "124  0.150376  0.389527  0.460098    212.jpg\n",
      "125  0.072075  0.509016  0.418909    213.jpg\n",
      "126  0.405961  0.566360  0.027678    214.jpg\n",
      "127  0.442763  0.543386  0.013851    215.jpg\n",
      "128  0.336087  0.623186  0.040727    216.jpg\n",
      "129  0.719904  0.265853  0.014242    217.jpg\n",
      "130  0.047212  0.473024  0.479764    218.jpg\n",
      "131  0.028727  0.657617  0.313656    219.jpg\n",
      "132  0.087033  0.348986  0.563981     22.jpg\n",
      "133  0.064975  0.671557  0.263468    220.jpg\n",
      "134  0.161370  0.501048  0.337582    221.jpg\n",
      "135  0.051013  0.686239  0.262748    222.jpg\n",
      "136  0.024377  0.035404  0.940219    223.jpg\n",
      "137  0.034372  0.374496  0.591133    224.jpg\n",
      "138  0.113111  0.757803  0.129086    225.jpg\n",
      "139  0.402413  0.409009  0.188578    226.jpg\n",
      "140  0.426959  0.459890  0.113151    227.jpg\n",
      "141  0.416402  0.575680  0.007918    228.jpg\n",
      "142  0.005027  0.238636  0.756336    229.jpg\n",
      "143  0.497233  0.335307  0.167460     23.jpg\n",
      "144  0.521214  0.433844  0.044942    230.jpg\n",
      "145  0.482439  0.505369  0.012192    231.jpg\n",
      "146  0.208458  0.765756  0.025786    232.jpg\n",
      "147  0.395964  0.506434  0.097602    233.jpg\n",
      "148  0.023066  0.361689  0.615244    234.jpg\n",
      "149  0.009793  0.958403  0.031803    235.jpg\n",
      "150  0.042465  0.594500  0.363035    236.jpg\n",
      "151  0.004480  0.878994  0.116526    237.jpg\n",
      "152  0.015520  0.256716  0.727764    238.jpg\n",
      "153  0.402113  0.551469  0.046418    239.jpg\n",
      "154  0.246405  0.516171  0.237423     24.jpg\n",
      "155  0.026978  0.110502  0.862520    240.jpg\n",
      "156  0.036631  0.950407  0.012962    241.jpg\n",
      "157  0.040057  0.855882  0.104060    242.jpg\n",
      "158  0.111615  0.661415  0.226971    244.jpg\n",
      "159  0.321885  0.653673  0.024442    245.jpg\n",
      "160  0.080441  0.808010  0.111549    246.jpg\n",
      "161  0.078331  0.136218  0.785451    247.jpg\n",
      "162  0.045009  0.705872  0.249120    248.jpg\n",
      "163  0.102593  0.451789  0.445617    249.jpg\n",
      "164  0.007949  0.808847  0.183204     25.jpg\n",
      "165  0.027693  0.321637  0.650670    250.jpg\n",
      "166  0.108156  0.332176  0.559668    251.jpg\n",
      "167  0.003843  0.467085  0.529073    252.jpg\n",
      "168  0.012946  0.522943  0.464111    253.jpg\n",
      "169  0.055930  0.385920  0.558150    254.jpg\n",
      "170  0.027625  0.866878  0.105497    255.jpg\n",
      "171  0.015044  0.296733  0.688223    256.jpg\n",
      "172  0.028478  0.930346  0.041176    257.jpg\n",
      "173  0.014592  0.231239  0.754169    258.jpg\n",
      "174  0.062843  0.747105  0.190052    259.jpg\n",
      "175  0.050587  0.919754  0.029659     26.jpg\n",
      "176  0.048925  0.933113  0.017962    260.jpg\n",
      "177  0.053057  0.925054  0.021890    261.jpg\n",
      "178  0.049474  0.685357  0.265168    262.jpg\n",
      "179  0.046238  0.914376  0.039386    263.jpg\n",
      "180  0.392913  0.421005  0.186082    264.jpg\n",
      "181  0.036757  0.291151  0.672091    265.jpg\n",
      "182  0.019007  0.574076  0.406917    266.jpg\n",
      "183  0.673138  0.301237  0.025625    267.jpg\n",
      "184  0.253459  0.716098  0.030443    268.jpg\n",
      "185  0.400083  0.580253  0.019664    269.jpg\n",
      "186  0.004951  0.447117  0.547932     27.jpg\n",
      "187  0.053225  0.811071  0.135704    270.jpg\n",
      "188  0.123671  0.797576  0.078753    271.jpg\n",
      "189  0.071194  0.777923  0.150883    272.jpg\n",
      "190  0.133118  0.338659  0.528223    273.jpg\n",
      "191  0.030813  0.756289  0.212898    274.jpg\n",
      "192  0.203284  0.725348  0.071368    275.jpg\n",
      "193  0.116423  0.202337  0.681240    276.jpg\n",
      "194  0.033595  0.221492  0.744914    278.jpg\n",
      "195  0.038659  0.249126  0.712215    279.jpg\n",
      "196  0.182430  0.774778  0.042792     28.jpg\n",
      "197  0.526962  0.450698  0.022339    280.jpg\n",
      "198  0.021645  0.524583  0.453772    281.jpg\n",
      "199  0.480132  0.431362  0.088506    282.jpg\n",
      "200  0.007862  0.488854  0.503284    283.jpg\n",
      "201  0.330820  0.645354  0.023826    285.jpg\n",
      "202  0.114861  0.757165  0.127974    286.jpg\n",
      "203  0.019640  0.639915  0.340445    287.jpg\n",
      "204  0.047872  0.820246  0.131882    288.jpg\n",
      "205  0.286750  0.683895  0.029355    289.jpg\n",
      "206  0.035696  0.864061  0.100243     29.jpg\n",
      "207  0.007161  0.133025  0.859815    290.jpg\n",
      "208  0.122860  0.863714  0.013427    291.jpg\n",
      "209  0.007087  0.962334  0.030579    292.jpg\n",
      "210  0.732750  0.174122  0.093127    293.jpg\n",
      "211  0.041824  0.940943  0.017233    294.jpg\n",
      "212  0.103486  0.805425  0.091089    295.jpg\n",
      "213  0.299614  0.540269  0.160117    296.jpg\n",
      "214  0.488098  0.475052  0.036850    297.jpg\n",
      "215  0.004256  0.669621  0.326122    298.jpg\n",
      "216  0.078175  0.469931  0.451894    299.jpg\n",
      "217  0.069308  0.710795  0.219897      3.jpg\n",
      "218  0.148568  0.592834  0.258598     30.jpg\n",
      "219  0.281839  0.683456  0.034705    300.jpg\n",
      "220  0.014120  0.594363  0.391517    301.jpg\n",
      "221  0.080058  0.334760  0.585182    302.jpg\n",
      "222  0.687478  0.282528  0.029994    303.jpg\n",
      "223  0.982913  0.015845  0.001241    304.jpg\n",
      "224  0.031232  0.712352  0.256416    305.jpg\n",
      "225  0.041719  0.890086  0.068195    306.jpg\n",
      "226  0.045772  0.166725  0.787503    307.jpg\n",
      "227  0.248302  0.714445  0.037253    308.jpg\n",
      "228  0.018609  0.620143  0.361248    309.jpg\n",
      "229  0.003153  0.800503  0.196344     31.jpg\n",
      "230  0.099405  0.579187  0.321408    310.jpg\n",
      "231  0.003485  0.105587  0.890927    311.jpg\n",
      "232  0.042875  0.644636  0.312490    312.jpg\n",
      "233  0.138284  0.618299  0.243417    313.jpg\n",
      "234  0.202135  0.781348  0.016516    314.jpg\n",
      "235  0.294479  0.437409  0.268112    315.jpg\n",
      "236  0.386924  0.282524  0.330552    316.jpg\n",
      "237  0.116784  0.586014  0.297203    317.jpg\n",
      "238  0.446983  0.442313  0.110704    318.jpg\n",
      "239  0.103719  0.648106  0.248175    319.jpg\n",
      "240  0.071522  0.883789  0.044690     32.jpg\n",
      "241  0.064965  0.834613  0.100423    320.jpg\n",
      "242  0.123311  0.756978  0.119712    321.jpg\n",
      "243  0.306479  0.345785  0.347736    322.jpg\n",
      "244  0.035879  0.637660  0.326461    323.jpg\n",
      "245  0.018257  0.792464  0.189279    324.jpg\n",
      "246  0.019173  0.064162  0.916665    325.jpg\n",
      "247  0.007990  0.192649  0.799361    326.jpg\n",
      "248  0.114189  0.304566  0.581245    327.jpg\n",
      "249  0.181606  0.606873  0.211521    328.jpg\n",
      "250  0.031346  0.858739  0.109916    329.jpg\n",
      "251  0.351662  0.490487  0.157851     33.jpg\n",
      "252  0.012341  0.426727  0.560932    330.jpg\n",
      "253  0.220701  0.731246  0.048053    332.jpg\n",
      "254  0.135457  0.498915  0.365628    333.jpg\n",
      "255  0.015160  0.538159  0.446682    334.jpg\n",
      "256  0.022435  0.466017  0.511548    335.jpg\n",
      "257  0.130738  0.845618  0.023644    336.jpg\n",
      "258  0.229566  0.311746  0.458688    337.jpg\n",
      "259  0.132726  0.569766  0.297508    338.jpg\n",
      "260  0.009230  0.131062  0.859709    339.jpg\n",
      "261  0.130799  0.748155  0.121045     34.jpg\n",
      "262  0.402104  0.580269  0.017626    340.jpg\n",
      "263  0.061125  0.607402  0.331474    341.jpg\n",
      "264  0.021510  0.860156  0.118334    342.jpg\n",
      "265  0.008951  0.392514  0.598536    343.jpg\n",
      "266  0.224955  0.629057  0.145988    344.jpg\n",
      "267  0.022646  0.630076  0.347277    345.jpg\n",
      "268  0.010568  0.520476  0.468956    346.jpg\n",
      "269  0.612041  0.367843  0.020116    347.jpg\n",
      "270  0.033832  0.051636  0.914532    348.jpg\n",
      "271  0.060013  0.649627  0.290360    349.jpg\n",
      "272  0.020293  0.620006  0.359701     35.jpg\n",
      "273  0.002928  0.323953  0.673120    350.jpg\n",
      "274  0.038096  0.858047  0.103858    351.jpg\n",
      "275  0.044809  0.794270  0.160922    352.jpg\n",
      "276  0.091915  0.364800  0.543285    353.jpg\n",
      "277  0.026070  0.938886  0.035044    354.jpg\n",
      "278  0.832832  0.159706  0.007462    355.jpg\n",
      "279  0.212253  0.746549  0.041198    356.jpg\n",
      "280  0.066817  0.737253  0.195930    357.jpg\n",
      "281  0.064747  0.908859  0.026394    358.jpg\n",
      "282  0.027010  0.861815  0.111175    359.jpg\n",
      "283  0.113211  0.827921  0.058868     36.jpg\n",
      "284  0.144396  0.752782  0.102822    360.jpg\n",
      "285  0.029455  0.140518  0.830027    361.jpg\n",
      "286  0.011877  0.826683  0.161441    362.jpg\n",
      "287  0.029028  0.420665  0.550307    363.jpg\n",
      "288  0.032414  0.407426  0.560160    364.jpg\n",
      "289  0.013805  0.414304  0.571891    365.jpg\n",
      "290  0.096205  0.304199  0.599595    366.jpg\n",
      "291  0.574136  0.372303  0.053561    367.jpg\n",
      "292  0.032062  0.476001  0.491937    368.jpg\n",
      "293  0.047105  0.686584  0.266311    369.jpg\n",
      "294  0.134565  0.454778  0.410657     37.jpg\n",
      "295  0.213561  0.320842  0.465597    370.jpg\n",
      "296  0.150406  0.818640  0.030954    371.jpg\n",
      "297  0.025087  0.155520  0.819393    372.jpg\n",
      "298  0.581478  0.333614  0.084908    373.jpg\n",
      "299  0.343919  0.159420  0.496661    374.jpg\n",
      "300  0.035462  0.675843  0.288695    375.jpg\n",
      "301  0.017649  0.202812  0.779540    376.jpg\n",
      "302  0.101896  0.440210  0.457894    377.jpg\n",
      "303  0.058051  0.649190  0.292759    378.jpg\n",
      "304  0.004356  0.220241  0.775403    379.jpg\n",
      "305  0.017887  0.934220  0.047893     38.jpg\n",
      "306  0.007708  0.669070  0.323222    380.jpg\n",
      "307  0.114308  0.497583  0.388109    381.jpg\n",
      "308  0.338158  0.642318  0.019524    382.jpg\n",
      "309  0.217312  0.755089  0.027599    383.jpg\n",
      "310  0.004720  0.087445  0.907835    384.jpg\n",
      "311  0.011033  0.734141  0.254826    385.jpg\n",
      "312  0.141330  0.209585  0.649085    386.jpg\n",
      "313  0.019287  0.435267  0.545446    387.jpg\n",
      "314  0.066858  0.643046  0.290095    388.jpg\n",
      "315  0.425193  0.399324  0.175484    389.jpg\n",
      "316  0.050911  0.695891  0.253198     39.jpg\n",
      "317  0.088146  0.822427  0.089427    390.jpg\n",
      "318  0.039577  0.532707  0.427716    391.jpg\n",
      "319  0.165847  0.310376  0.523777    392.jpg\n",
      "320  0.038711  0.648988  0.312301    393.jpg\n",
      "321  0.109735  0.694946  0.195319    394.jpg\n",
      "322  0.288030  0.550575  0.161395    395.jpg\n",
      "323  0.038775  0.944801  0.016425    396.jpg\n",
      "324  0.334666  0.611051  0.054283    397.jpg\n",
      "325  0.004382  0.192459  0.803159    398.jpg\n",
      "326  0.010877  0.320171  0.668952    399.jpg\n",
      "327  0.004572  0.278002  0.717426      4.jpg\n",
      "328  0.039436  0.139703  0.820861     40.jpg\n",
      "329  0.084063  0.543293  0.372644    400.jpg\n",
      "330  0.006863  0.730388  0.262749    401.jpg\n",
      "331  0.706695  0.266498  0.026807    402.jpg\n",
      "332  0.148807  0.843209  0.007984    403.jpg\n",
      "333  0.328019  0.645766  0.026215    404.jpg\n",
      "334  0.664396  0.318389  0.017215    405.jpg\n",
      "335  0.035258  0.310301  0.654442    406.jpg\n",
      "336  0.154267  0.832077  0.013656    407.jpg\n",
      "337  0.009556  0.428564  0.561879    408.jpg\n",
      "338  0.063979  0.572156  0.363865    409.jpg\n",
      "339  0.040440  0.597616  0.361944     41.jpg\n",
      "340  0.005406  0.773050  0.221544    410.jpg\n",
      "341  0.039821  0.194374  0.765805    411.jpg\n",
      "342  0.030685  0.405575  0.563740    412.jpg\n",
      "343  0.167435  0.507477  0.325088    413.jpg\n",
      "344  0.050671  0.531288  0.418041    414.jpg\n",
      "345  0.290593  0.658294  0.051113    415.jpg\n",
      "346  0.091686  0.887476  0.020838    416.jpg\n",
      "347  0.221715  0.742413  0.035872    417.jpg\n",
      "348  0.303285  0.371401  0.325315    418.jpg\n",
      "349  0.092145  0.879898  0.027957    419.jpg\n",
      "350  0.280442  0.422214  0.297343     42.jpg\n",
      "351  0.002114  0.622948  0.374938    420.jpg\n",
      "352  0.298626  0.547159  0.154215    421.jpg\n",
      "353  0.050129  0.704612  0.245260    422.jpg\n",
      "354  0.041333  0.456708  0.501960    423.jpg\n",
      "355  0.006498  0.066231  0.927271    424.jpg\n",
      "356  0.003156  0.471774  0.525070    425.jpg\n",
      "357  0.041583  0.770135  0.188282    426.jpg\n",
      "358  0.398019  0.458768  0.143212    427.jpg\n",
      "359  0.038094  0.612912  0.348994    428.jpg\n",
      "360  0.010541  0.543507  0.445952    429.jpg\n",
      "361  0.005906  0.196119  0.797975     43.jpg\n",
      "362  0.136944  0.737051  0.126005    430.jpg\n",
      "363  0.297946  0.688581  0.013473    431.jpg\n",
      "364  0.023281  0.332823  0.643896    432.jpg\n",
      "365  0.112070  0.469435  0.418496    433.jpg\n",
      "366  0.008486  0.149713  0.841802    434.jpg\n",
      "367  0.003982  0.330185  0.665833    435.jpg\n",
      "368  0.089966  0.885890  0.024143    436.jpg\n",
      "369  0.126348  0.418458  0.455194    437.jpg\n",
      "370  0.130844  0.842861  0.026295    438.jpg\n",
      "371  0.138595  0.588549  0.272857    439.jpg\n",
      "372  0.004362  0.885633  0.110005     44.jpg\n",
      "373  0.019452  0.170420  0.810128    440.jpg\n",
      "374  0.036717  0.496059  0.467224    441.jpg\n",
      "375  0.350064  0.594049  0.055887    442.jpg\n",
      "376  0.083585  0.385963  0.530452    443.jpg\n",
      "377  0.506240  0.301506  0.192254    444.jpg\n",
      "378  0.037473  0.944674  0.017853    445.jpg\n",
      "379  0.042357  0.723255  0.234388    446.jpg\n",
      "380  0.003079  0.900860  0.096061    447.jpg\n",
      "381  0.133494  0.541520  0.324986    448.jpg\n",
      "382  0.042484  0.607752  0.349763    449.jpg\n",
      "383  0.129680  0.590506  0.279813     45.jpg\n",
      "384  0.062852  0.695455  0.241693    450.jpg\n",
      "385  0.044011  0.936473  0.019516    451.jpg\n",
      "386  0.020706  0.424490  0.554804    452.jpg\n",
      "387  0.294021  0.689130  0.016849    453.jpg\n",
      "388  0.027279  0.600609  0.372112    454.jpg\n",
      "389  0.641023  0.338967  0.020010    455.jpg\n",
      "390  0.022053  0.956313  0.021634    456.jpg\n",
      "391  0.409370  0.575628  0.015002    457.jpg\n",
      "392  0.211486  0.300096  0.488418    458.jpg\n",
      "393  0.090723  0.885545  0.023732    459.jpg\n",
      "394  0.010504  0.929222  0.060274     46.jpg\n",
      "395  0.018014  0.037674  0.944312    461.jpg\n",
      "396  0.188425  0.761046  0.050530    462.jpg\n",
      "397  0.455787  0.451258  0.092955    463.jpg\n",
      "398  0.254621  0.722343  0.023036    464.jpg\n",
      "399  0.124995  0.688230  0.186775    465.jpg\n",
      "400  0.200271  0.730711  0.069019    466.jpg\n",
      "401  0.051485  0.785983  0.162532    467.jpg\n",
      "402  0.107514  0.744969  0.147517    468.jpg\n",
      "403  0.006507  0.792053  0.201440    469.jpg\n",
      "404  0.086046  0.903116  0.010837     47.jpg\n",
      "405  0.049500  0.924189  0.026312    470.jpg\n",
      "406  0.610538  0.379091  0.010371    471.jpg\n",
      "407  0.008740  0.104301  0.886959    472.jpg\n",
      "408  0.015527  0.540240  0.444233    473.jpg\n",
      "409  0.086645  0.491690  0.421665    474.jpg\n",
      "410  0.062144  0.644653  0.293203    475.jpg\n",
      "411  0.011006  0.977887  0.011107    476.jpg\n",
      "412  0.683759  0.293947  0.022294    477.jpg\n",
      "413  0.029116  0.926572  0.044312    478.jpg\n",
      "414  0.163396  0.407416  0.429188    479.jpg\n",
      "415  0.004045  0.130896  0.865059     48.jpg\n",
      "416  0.013632  0.081607  0.904761    480.jpg\n",
      "417  0.007925  0.817151  0.174923    481.jpg\n",
      "418  0.066639  0.909594  0.023767    482.jpg\n",
      "419  0.024886  0.970286  0.004829    483.jpg\n",
      "420  0.317103  0.625355  0.057542    484.jpg\n",
      "421  0.011965  0.225629  0.762406    485.jpg\n",
      "422  0.041246  0.621280  0.337475    486.jpg\n",
      "423  0.738495  0.115681  0.145824    487.jpg\n",
      "424  0.066622  0.223331  0.710047    488.jpg\n",
      "425  0.068682  0.878047  0.053271    489.jpg\n",
      "426  0.019425  0.473541  0.507035     49.jpg\n",
      "427  0.092292  0.776616  0.131092    490.jpg\n",
      "428  0.009743  0.886397  0.103860    491.jpg\n",
      "429  0.174486  0.759859  0.065655    492.jpg\n",
      "430  0.274734  0.581571  0.143695    493.jpg\n",
      "431  0.010386  0.480025  0.509589    494.jpg\n",
      "432  0.019142  0.707806  0.273051    495.jpg\n",
      "433  0.068938  0.176831  0.754232    496.jpg\n",
      "434  0.148218  0.721386  0.130396    497.jpg\n",
      "435  0.051695  0.499549  0.448756    498.jpg\n",
      "436  0.542400  0.172288  0.285312    499.jpg\n",
      "437  0.115696  0.859510  0.024794      5.jpg\n",
      "438  0.010618  0.449245  0.540137     50.jpg\n",
      "439  0.181593  0.560338  0.258069    500.jpg\n",
      "440  0.252227  0.719911  0.027863    501.jpg\n",
      "441  0.017623  0.538198  0.444179    502.jpg\n",
      "442  0.023538  0.856995  0.119467    503.jpg\n",
      "443  0.043906  0.254569  0.701525    504.jpg\n",
      "444  0.211375  0.721023  0.067602    505.jpg\n",
      "445  0.197777  0.766362  0.035861    506.jpg\n",
      "446  0.060765  0.612846  0.326389    507.jpg\n",
      "447  0.006543  0.381708  0.611748    508.jpg\n",
      "448  0.304681  0.152452  0.542867    509.jpg\n",
      "449  0.508271  0.467566  0.024163     51.jpg\n",
      "450  0.020796  0.661853  0.317351    510.jpg\n",
      "451  0.277863  0.510026  0.212111    511.jpg\n",
      "452  0.009996  0.382847  0.607156     52.jpg\n",
      "453  0.001625  0.509705  0.488670     53.jpg\n",
      "454  0.013874  0.696908  0.289217     54.jpg\n",
      "455  0.228678  0.506636  0.264686     55.jpg\n",
      "456  0.700856  0.265287  0.033858     56.jpg\n",
      "457  0.071122  0.695050  0.233828     57.jpg\n",
      "458  0.123428  0.827703  0.048869     58.jpg\n",
      "459  0.494916  0.487455  0.017629     59.jpg\n",
      "460  0.087633  0.804753  0.107615      6.jpg\n",
      "461  0.040861  0.924518  0.034621     60.jpg\n",
      "462  0.405435  0.517564  0.077001     61.jpg\n",
      "463  0.049926  0.889200  0.060874     62.jpg\n",
      "464  0.077841  0.610383  0.311776     63.jpg\n",
      "465  0.847506  0.141580  0.010914     64.jpg\n",
      "466  0.473609  0.508388  0.018003     65.jpg\n",
      "467  0.166670  0.565938  0.267393     66.jpg\n",
      "468  0.014919  0.188354  0.796726     67.jpg\n",
      "469  0.049577  0.636596  0.313827     68.jpg\n",
      "470  0.027025  0.743182  0.229792     69.jpg\n",
      "471  0.012880  0.602701  0.384419      7.jpg\n",
      "472  0.147493  0.824209  0.028298     70.jpg\n",
      "473  0.621671  0.311919  0.066410     71.jpg\n",
      "474  0.080663  0.906300  0.013037     72.jpg\n",
      "475  0.034690  0.895810  0.069500     73.jpg\n",
      "476  0.180888  0.389228  0.429884     74.jpg\n",
      "477  0.350306  0.574790  0.074904     75.jpg\n",
      "478  0.178621  0.561957  0.259421     76.jpg\n",
      "479  0.106681  0.548128  0.345191     77.jpg\n",
      "480  0.007110  0.042828  0.950062     78.jpg\n",
      "481  0.184582  0.779737  0.035681     79.jpg\n",
      "482  0.007098  0.356453  0.636449      8.jpg\n",
      "483  0.396006  0.328477  0.275518     80.jpg\n",
      "484  0.682943  0.276911  0.040146     81.jpg\n",
      "485  0.271427  0.676499  0.052074     82.jpg\n",
      "486  0.164512  0.449816  0.385672     83.jpg\n",
      "487  0.275890  0.675294  0.048816     84.jpg\n",
      "488  0.173189  0.811872  0.014938     85.jpg\n",
      "489  0.059924  0.478813  0.461263     86.jpg\n",
      "490  0.032753  0.240289  0.726957     87.jpg\n",
      "491  0.120257  0.869545  0.010197     88.jpg\n",
      "492  0.428670  0.398083  0.173247     89.jpg\n",
      "493  0.357671  0.352992  0.289337      9.jpg\n",
      "494  0.059032  0.650378  0.290590     90.jpg\n",
      "495  0.564143  0.415278  0.020578     91.jpg\n",
      "496  0.023204  0.854079  0.122718     92.jpg\n",
      "497  0.051043  0.532327  0.416630     93.jpg\n",
      "498  0.099339  0.684486  0.216175     94.jpg\n",
      "499  0.014452  0.622861  0.362686     95.jpg\n",
      "500  0.015734  0.960348  0.023918     96.jpg\n",
      "501  0.130859  0.846388  0.022753     97.jpg\n",
      "502  0.032642  0.954792  0.012566     98.jpg\n",
      "503  0.019946  0.099282  0.880772     99.jpg\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    test(isTrue = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
